{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XcHO9PljuZHw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DmmbYJLhuZH_"
   },
   "outputs": [],
   "source": [
    "# Custom function for print a sentence\n",
    "def goodPrint(a):\n",
    "    x = \"\"\n",
    "    for i in range(len(a)):\n",
    "        x += a[i]+\" \"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jnO94rlfuZIL"
   },
   "source": [
    "## Downloading nltk corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "9ahWEusT1Zy1",
    "outputId": "1430b2f1-8981-42ab-ff6b-9e59bdc5e2ae"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "QnxVK_ZiuZIN",
    "outputId": "f6d9d098-c9af-4ab2-dce3-9b5c58827217"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cess_esp to\n",
      "[nltk_data]     C:\\Users\\jempool\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cess_esp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('cess_esp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KDWxgopZuZIY"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import cess_esp\n",
    "\n",
    "tagged_sentences = cess_esp.tagged_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxL9N4KiuZIf"
   },
   "source": [
    "## We get a corpus with sentences and the tag for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "TUzTTm6RuZIh",
    "outputId": "e07f656a-a50f-4eec-e29a-a274f452bf95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('El', 'da0ms0'), ('grupo', 'ncms000'), ('estatal', 'aq0cs0'), ('Electricité_de_France', 'np00000'), ('-Fpa-', 'Fpa'), ('EDF', 'np00000'), ('-Fpt-', 'Fpt'), ('anunció', 'vmis3s0'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da0fs0'), ('compra', 'ncfs000'), ('del', 'spcms'), ('51_por_ciento', 'Zp'), ('de', 'sps00'), ('la', 'da0fs0'), ('empresa', 'ncfs000'), ('mexicana', 'aq0fs0'), ('Electricidad_Águila_de_Altamira', 'np00000'), ('-Fpa-', 'Fpa'), ('EAA', 'np00000'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq0fsp'), ('por', 'sps00'), ('el', 'da0ms0'), ('japonés', 'aq0ms0'), ('Mitsubishi_Corporation', 'np00000'), ('para', 'sps00'), ('poner_en_marcha', 'vmn0000'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('de', 'sps00'), ('495', 'Z'), ('megavatios', 'ncmp000'), ('.', 'Fp')] \n",
      "\n",
      "Tagged sentences:  6030\n",
      "Tagged words: 192686\n"
     ]
    }
   ],
   "source": [
    "print(tagged_sentences[0], \"\\n\")\n",
    "print(\"Tagged sentences: \", len(tagged_sentences))\n",
    "print(\"Tagged words:\", len(cess_esp.tagged_words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j3YecegQuZIp"
   },
   "source": [
    "## Splitting Sentence and tag inside two different arrays, later write 2 files for the 2 arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "UsRehvjFuZIr",
    "outputId": "66b4a9e3-fe0d-4b0a-ca3f-a66d42ad89a2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "sentences, tagss = [], []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    tagss.append(np.array(tags))\n",
    "    sentences.append(np.array(sentence))\n",
    "        \n",
    "# with open(\"sentences.txt\", \"wb\") as fp:\n",
    "#     pickle.dump(sentences, fp)\n",
    "    \n",
    "# with open(\"tags.txt\", \"wb\") as fp:\n",
    "#     pickle.dump(tagss, fp)\n",
    "\n",
    "# print(\"cess_esp sentence 0\\n\", sentences[0], \"\\n\")\n",
    "# print(\"cess_esp tags 0\\n\", tagss[0], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S7qhFY6quZIz"
   },
   "source": [
    "## Printing a sample of the arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "IhQ_o1RruZI1",
    "outputId": "deb57770-9f3d-4553-e691-2530fdeb6e5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6030\n",
      "\n",
      "Words on the first sentence: 40\n",
      "\n",
      "El grupo estatal Electricité_de_France -Fpa- EDF -Fpt- anunció hoy , jueves , la compra del 51_por_ciento de la empresa mexicana Electricidad_Águila_de_Altamira -Fpa- EAA -Fpt- , creada por el japonés Mitsubishi_Corporation para poner_en_marcha una central de gas de 495 megavatios . \n",
      "\n",
      "tags of the first sentence: 40\n",
      "\n",
      "['da0ms0' 'ncms000' 'aq0cs0' 'np00000' 'Fpa' 'np00000' 'Fpt' 'vmis3s0'\n",
      " 'rg' 'Fc' 'W' 'Fc' 'da0fs0' 'ncfs000' 'spcms' 'Zp' 'sps00' 'da0fs0'\n",
      " 'ncfs000' 'aq0fs0' 'np00000' 'Fpa' 'np00000' 'Fpt' 'Fc' 'aq0fsp' 'sps00'\n",
      " 'da0ms0' 'aq0ms0' 'np00000' 'sps00' 'vmn0000' 'di0fs0' 'ncfs000' 'sps00'\n",
      " 'ncms000' 'sps00' 'Z' 'ncmp000' 'Fp']\n"
     ]
    }
   ],
   "source": [
    "print(str(len(sentences)) + \"\\n\")\n",
    "print(\"Words on the first sentence: \"+ str(len(sentences[0])) +\"\\n\")\n",
    "print(goodPrint(sentences[0])+ \"\\n\")\n",
    "print(\"tags of the first sentence: \"+ str(len(tagss[0])) +\"\\n\" )\n",
    "print(tagss[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ACHtdEPRuZI6"
   },
   "source": [
    "## Percentages for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7OizRh02uZI8"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "(training_sentences, \n",
    " test_sentences, \n",
    " training_tags, \n",
    " test_tags) = train_test_split(sentences, tagss, test_size=0.2)\n",
    "\n",
    "(train_sentences, \n",
    " eval_sentences, \n",
    " train_tags, \n",
    " eval_tags) = train_test_split(training_sentences, training_tags, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "R-knaWomuZJD",
    "outputId": "c8dde008-3499-478a-98fb-85634974783a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_sentences: 4824\n",
      "train_sentences: 3618\n",
      "test_sentences: 1206\n",
      "eval_sentences: 1206\n",
      "\n",
      "first train_sentence ( 25 )\n",
      " ['Además' ',' 'hay' 'una' 'cosa' 'que' 'tiene' 'esta' 'casita' 'y' 'que'\n",
      " 'no' 'se' 'puede' 'trasladal' ':' 'el' '-' 'fleivor' '-' '-Fpa-' 'el'\n",
      " 'sabor' '-Fpt-' '.']\n",
      "\n",
      "first test_sentence ( 31 )\n",
      " ['Un' 'fallo' 'en' 'el' 'mando' 'a' 'distancia' 'hizo' 'fracasar' 'el'\n",
      " 'atentado' 'planeado' 'por' 'ese' 'comando' ',' 'que' 'pretendía' 'volar'\n",
      " 'un' 'furgón' 'ocupado' 'por' '24' 'militares' ',' 'según' 'la'\n",
      " 'justicia' 'española' '.']\n",
      "\n",
      "first eval_sentence ( 37 )\n",
      " ['Según' 'el' 'Post' ',' 'el' 'cierre' 'de' 'la' 'planta' 'de' 'Kellogs'\n",
      " 'se' 'debió' 'a' 'la' 'imposibilidad' 'de' 'que' 'los' 'grandes'\n",
      " 'proveedores' 'de' 'granos' 'certificaran' 'que' 'su' 'maíz' 'no'\n",
      " 'estaba' '\"' 'adulterado' '\"' 'con' 'la' 'variedad' 'StartLink' '.']\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "training_tags: 4824\n",
      "train_tags: 3618\n",
      "test_tags: 1206\n",
      "eval_tags: 1206\n",
      "\n",
      "first train_tags ( 25 )\n",
      " ['rg' 'Fc' 'vaip3s0' 'di0fs0' 'ncfs000' 'pr0cn000' 'vmip3s0' 'dd0fs0'\n",
      " 'ncfs000' 'cc' 'pr0cn000' 'rn' 'p0000000' 'vmip3s0' 'vmn0000' 'Fd'\n",
      " 'da0ms0' 'Fg' 'ncms000' 'Fg' 'Fpa' 'da0ms0' 'ncms000' 'Fpt' 'Fp']\n",
      "\n",
      "first test_tags ( 31 )\n",
      " ['di0ms0' 'ncms000' 'sps00' 'da0ms0' 'ncms000' 'sps00' 'ncfs000' 'vmis3s0'\n",
      " 'vmn0000' 'da0ms0' 'ncms000' 'aq0msp' 'sps00' 'dd0ms0' 'ncms000' 'Fc'\n",
      " 'pr0cn000' 'vmii3s0' 'vmn0000' 'di0ms0' 'ncms000' 'aq0msp' 'sps00' 'Z'\n",
      " 'nccp000' 'Fc' 'sps00' 'da0fs0' 'ncfs000' 'aq0fs0' 'Fp']\n",
      "\n",
      "first eval_tags ( 37 )\n",
      " ['sps00' 'da0ms0' 'np0000o' 'Fc' 'da0ms0' 'ncms000' 'sps00' 'da0fs0'\n",
      " 'ncfs000' 'sps00' 'np0000o' 'p0300000' 'vmis3s0' 'sps00' 'da0fs0'\n",
      " 'ncfs000' 'sps00' 'cs' 'da0mp0' 'aq0cp0' 'ncmp000' 'sps00' 'ncmp000'\n",
      " 'vmsi3p0' 'cs' 'dp3cs0' 'ncms000' 'rn' 'vmii3s0' 'Fe' 'aq0msp' 'Fe'\n",
      " 'sps00' 'da0fs0' 'ncfs000' 'np0000a' 'Fp']\n"
     ]
    }
   ],
   "source": [
    "print(\"training_sentences: \" + str(len(training_sentences)))\n",
    "print(\"train_sentences: \" + str(len(train_sentences)))\n",
    "print(\"test_sentences: \" + str(len(test_sentences)))\n",
    "print(\"eval_sentences: \" + str(len(eval_sentences)) + \"\\n\")\n",
    "\n",
    "print(\"first train_sentence (\",len(train_sentences[0]) , \")\\n\", train_sentences[0])\n",
    "print(\"\\nfirst test_sentence (\",len(test_sentences[0]) , \")\\n\", test_sentences[0])\n",
    "print(\"\\nfirst eval_sentence (\",len(eval_sentences[0]) , \")\\n\", eval_sentences[0])\n",
    "\n",
    "print(\"\\n---------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"\\ntraining_tags: \" + str(len(training_sentences)))\n",
    "print(\"train_tags: \" + str(len(train_tags)))\n",
    "print(\"test_tags: \" + str(len(test_tags)))\n",
    "print(\"eval_tags: \" + str(len(eval_tags)) + \"\\n\")\n",
    "\n",
    "print(\"first train_tags (\",len(train_tags[0]) , \")\\n\", train_tags[0])\n",
    "print(\"\\nfirst test_tags (\",len(test_tags[0]) , \")\\n\", test_tags[0])\n",
    "print(\"\\nfirst eval_tags (\",len(eval_tags[0]) , \")\\n\", eval_tags[0])\n",
    "\n",
    "#print(len(train_sentences[0]), len(train_sentences[1]), len(train_sentences[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHecruQluZJK"
   },
   "source": [
    "## Building dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "jLlvGcXpuZJL",
    "outputId": "02dada7d-a1d2-4103-94a6-016b7ad74486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24499\n",
      "291\n"
     ]
    }
   ],
   "source": [
    "words, tagsss = set([]), set([])\n",
    " \n",
    "for s in (train_sentences + eval_sentences + test_sentences):\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    "\n",
    "for ts in (train_tags + eval_tags + test_tags):\n",
    "    for t in ts:\n",
    "        tagsss.add(t)\n",
    "\n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 2 for i, t in enumerate(list(tagsss))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding\n",
    "tag2index['-OOV-'] = 1  # The special value used to OOVs\n",
    "\n",
    "print (len(word2index))\n",
    "print (len(tag2index))\n",
    "\n",
    "# PRINT THIS FOR CHECKING *************************\n",
    "# print(word2index)\n",
    "# print(tag2index)\n",
    "\n",
    "np.save('word2index.npy', word2index)\n",
    "np.save('tag2index.npy', tag2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6qh3so4uZJR"
   },
   "source": [
    "## Parsing Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YcKOFMa4uZJS"
   },
   "outputs": [],
   "source": [
    "train_sentences_X, eval_sentences_X, test_sentences_X, train_tags_y, eval_tags_y, test_tags_y = [], [], [], [], [], []\n",
    "\n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    "\n",
    "for s in eval_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    eval_sentences_X.append(s_int)\n",
    "\n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    "\n",
    "for s in train_tags:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(tag2index[w])\n",
    "        except KeyError:\n",
    "            s_int.append(tag2index['-OOV-'])\n",
    "            \n",
    "    train_tags_y.append(s_int)\n",
    "\n",
    "for s in eval_tags:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(tag2index[w])\n",
    "        except KeyError:\n",
    "            s_int.append(tag2index['-OOV-'])\n",
    "            \n",
    "    eval_tags_y.append(s_int)\n",
    "\n",
    "for s in test_tags:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(tag2index[w])\n",
    "        except KeyError:\n",
    "            s_int.append(tag2index['-OOV-'])\n",
    "            \n",
    "    test_tags_y.append(s_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "SfgC-gRAuZJb",
    "outputId": "6eaae50c-e132-4e63-f91e-1a86e5bebfaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datos presentes en las Matrices con las transformaciones:\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\n",
      "train_sentences_X ( 3618 )\n",
      " [20524, 12364, 16834, 22392, 15897, 24297, 18522, 1499, 14159, 13865, 24297, 9236, 12895, 18678, 831, 20747, 14090, 10324, 21224, 10324, 6966, 14090, 9029, 22493, 5597]\n",
      "\n",
      "train_tags_y ( 3618 )\n",
      " [134, 257, 170, 71, 256, 151, 216, 70, 256, 244, 151, 14, 275, 216, 65, 5, 145, 18, 80, 18, 180, 145, 80, 265, 146]\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\n",
      "eval_sentences_X ( 1206 )\n",
      " [18397, 14090, 15132, 12364, 14090, 196, 3827, 371, 14341, 3827, 3734, 12895, 11546, 5417, 371, 20052, 3827, 24297, 4310, 11343, 22744, 3827, 1762, 9436, 24297, 21124, 7306, 9236, 7898, 7315, 520, 7315, 2120, 371, 15791, 9807, 5597]\n",
      "\n",
      "eval_tags_y ( 1206 )\n",
      " [66, 145, 95, 257, 145, 80, 66, 243, 256, 66, 95, 73, 269, 66, 243, 256, 66, 239, 23, 20, 75, 66, 75, 76, 239, 210, 80, 14, 193, 81, 211, 81, 66, 243, 256, 183, 146]\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\n",
      "test_sentences_X ( 1206 )\n",
      " [468, 16450, 3059, 14090, 1680, 5417, 1132, 17560, 8048, 14090, 6591, 3013, 8852, 3052, 17406, 12364, 24297, 7027, 15660, 468, 5978, 6929, 8852, 8068, 792, 12364, 18397, 371, 20804, 14539, 5597]\n",
      "\n",
      "test_tags_y ( 1206 )\n",
      " [68, 80, 66, 145, 80, 66, 256, 269, 65, 145, 80, 211, 66, 150, 80, 257, 151, 193, 65, 68, 80, 211, 66, 148, 92, 257, 66, 243, 256, 155, 146] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDatos presentes en las Matrices con las transformaciones:\")\n",
    "print(\"\\n----------------------------------------------------------------\")\n",
    "\n",
    "print(\"\\ntrain_sentences_X (\", len(train_sentences_X),\")\\n\", train_sentences_X[0])\n",
    "print(\"\\ntrain_tags_y (\", len(train_tags_y),\")\\n\", train_tags_y[0])\n",
    "print(\"\\n----------------------------------------------------------------\")\n",
    "\n",
    "print(\"\\neval_sentences_X (\", len(eval_sentences_X),\")\\n\", eval_sentences_X[0])\n",
    "print(\"\\neval_tags_y (\", len(eval_tags_y),\")\\n\", eval_tags_y[0])\n",
    "print(\"\\n----------------------------------------------------------------\")\n",
    "\n",
    "print(\"\\ntest_sentences_X (\", len(test_sentences_X),\")\\n\", test_sentences_X[0])\n",
    "print(\"\\ntest_tags_y (\", len(test_tags_y),\")\\n\", test_tags_y[0], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ub7O0xqEuZJh"
   },
   "source": [
    "## Checking transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "ejAZBVYBuZJi",
    "outputId": "a512a15c-bde4-4418-b6a0-791aa34f694b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words on the first sentence:  25\n",
      "Además , hay una cosa que tiene esta casita y que no se puede trasladal : el - fleivor - -Fpa- el sabor -Fpt- . \n",
      "\n",
      "Words on the first sentence Parsed:  25\n",
      "[20524, 12364, 16834, 22392, 15897, 24297, 18522, 1499, 14159, 13865, 24297, 9236, 12895, 18678, 831, 20747, 14090, 10324, 21224, 10324, 6966, 14090, 9029, 22493, 5597]\n",
      "\n",
      "tags of the first sentence:  25\n",
      "['rg' 'Fc' 'vaip3s0' 'di0fs0' 'ncfs000' 'pr0cn000' 'vmip3s0' 'dd0fs0'\n",
      " 'ncfs000' 'cc' 'pr0cn000' 'rn' 'p0000000' 'vmip3s0' 'vmn0000' 'Fd'\n",
      " 'da0ms0' 'Fg' 'ncms000' 'Fg' 'Fpa' 'da0ms0' 'ncms000' 'Fpt' 'Fp']\n",
      "\n",
      "tags of the first sentence Parsed: 25\n",
      "[134, 257, 170, 71, 256, 151, 216, 70, 256, 244, 151, 14, 275, 216, 65, 5, 145, 18, 80, 18, 180, 145, 80, 265, 146]\n"
     ]
    }
   ],
   "source": [
    "print(\"Words on the first sentence: \", (len(train_sentences[0])))\n",
    "print(goodPrint(train_sentences[0])+ \"\\n\")\n",
    "\n",
    "print(\"Words on the first sentence Parsed: \", (len(train_sentences_X[0])))\n",
    "print(train_sentences_X[0])\n",
    "\n",
    "print(\"\\ntags of the first sentence: \", (len(train_tags[0])))\n",
    "print(train_tags[0])\n",
    "\n",
    "print(\"\\ntags of the first sentence Parsed: \"+str(len(train_tags_y[0])))\n",
    "print(train_tags_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mw78VGYxuZJp"
   },
   "source": [
    "## Sentence with more words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "HOCcNUJWuZJq",
    "outputId": "8c729b4d-fb82-4948-960d-49decf01e3cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_LENGTH_TRAIN:  149\n",
      "MAX_LENGTH_EVAL:  131\n",
      "MAX_LENGTH_TEST:  131\n",
      "\n",
      "MAX_LENGTH:  149\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH_TRAIN = len(max(train_sentences_X, key=len))\n",
    "MAX_LENGTH_EVAL = len(max(eval_sentences_X, key=len))\n",
    "MAX_LENGTH_TEST = len(max(test_sentences_X, key=len))\n",
    "\n",
    "print(\"MAX_LENGTH_TRAIN: \", MAX_LENGTH_TRAIN)\n",
    "print(\"MAX_LENGTH_EVAL: \", MAX_LENGTH_EVAL)\n",
    "print(\"MAX_LENGTH_TEST: \", MAX_LENGTH_TEST)\n",
    "\n",
    "MAX_LENGTH = max(MAX_LENGTH_TRAIN, MAX_LENGTH_EVAL, MAX_LENGTH_TEST)\n",
    "print(\"\\nMAX_LENGTH: \", MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFaD4D23uZJy"
   },
   "source": [
    "### Se procede a Normalizar las matrices para que todas contengan el mismo numero de columans, con la longitud maxima de palabras encontradas anteriormente, esto se logra agregando ceros a la derecha en las posiciones que hacen falta en el vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zw-x-jNZuZJz",
    "outputId": "1bf3cac9-91e1-413f-9844-73406eb4a2e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20524 12364 16834 22392 15897 24297 18522  1499 14159 13865 24297  9236\n",
      " 12895 18678   831 20747 14090 10324 21224 10324  6966 14090  9029 22493\n",
      "  5597     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0]\n",
      "[18397 14090 15132 12364 14090   196  3827   371 14341  3827  3734 12895\n",
      " 11546  5417   371 20052  3827 24297  4310 11343 22744  3827  1762  9436\n",
      " 24297 21124  7306  9236  7898  7315   520  7315  2120   371 15791  9807\n",
      "  5597     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0]\n",
      "[  468 16450  3059 14090  1680  5417  1132 17560  8048 14090  6591  3013\n",
      "  8852  3052 17406 12364 24297  7027 15660   468  5978  6929  8852  8068\n",
      "   792 12364 18397   371 20804 14539  5597     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0]\n",
      "[134 257 170  71 256 151 216  70 256 244 151  14 275 216  65   5 145  18\n",
      "  80  18 180 145  80 265 146   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0]\n",
      "[ 66 145  95 257 145  80  66 243 256  66  95  73 269  66 243 256  66 239\n",
      "  23  20  75  66  75  76 239 210  80  14 193  81 211  81  66 243 256 183\n",
      " 146   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0]\n",
      "[ 68  80  66 145  80  66 256 269  65 145  80 211  66 150  80 257 151 193\n",
      "  65  68  80 211  66 148  92 257  66 243 256 155 146   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "eval_sentences_X = pad_sequences(eval_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "eval_tags_y = pad_sequences(eval_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "print(train_sentences_X[0])\n",
    "print(eval_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(eval_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sWwZ3UuAuZJ3"
   },
   "source": [
    "## Definimos la funcion con la cual categorizaremos los tags y los covertiremos en vector One-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pmckYX7tuZJ4"
   },
   "outputs": [],
   "source": [
    "def to_categoricals(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)\n",
    "\n",
    "\n",
    "cat_train_tags_y = to_categoricals(train_tags_y, len(tag2index))\n",
    "cat_eval_tags_y  = to_categoricals(eval_tags_y, len(tag2index))\n",
    "cat_test_tags_y  = to_categoricals(test_tags_y, len(tag2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "acz-nVCRuZKE",
    "outputId": "2d8bf8de-0531-4a1c-86df-a6f2fb2370d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tags_y[0]\n",
      " [134 257 170  71 256 151 216  70 256 244 151  14 275 216  65   5 145  18\n",
      "  80  18 180 145  80 265 146   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0] \n",
      "\n",
      "cat_train_tags_y[0]\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]] \n",
      "\n",
      "cat_train_tags_y[0][0]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.] \n",
      "\n",
      "cat_train_tags_y[0][0]:  291\n",
      "tag2index:  291\n",
      "cat_test_tags_y 1206\n"
     ]
    }
   ],
   "source": [
    "print(\"train_tags_y[0]\\n\", train_tags_y[0], \"\\n\")\n",
    "print(\"cat_train_tags_y[0]\\n\", cat_train_tags_y[0], \"\\n\")\n",
    "print(\"cat_train_tags_y[0][0]\\n\", cat_train_tags_y[0][0], \"\\n\")\n",
    "\n",
    "print(\"cat_train_tags_y[0][0]: \", len(cat_train_tags_y[0][0]))\n",
    "print(\"tag2index: \", len(tag2index))\n",
    "print(\"cat_test_tags_y\", len(cat_test_tags_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-OmbJuwuZKK"
   },
   "source": [
    "# SBWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SnJLFn8tuZKM",
    "outputId": "9c1d58ab-9fc4-40ed-b95f-ea270c082438"
   },
   "outputs": [],
   "source": [
    "!ls drive/My\\ Drive/ | grep SBW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "2BZcrJX9uZKU",
    "outputId": "76c098f1-bbf3-4ffb-9d81-8b69deda188e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000653 word vectors.\n",
      "<class 'dict'>\n",
      "24499\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'SBW-vectors-300-min5.txt'\n",
    "'''\n",
    "This dataset contains 1,000,653 word embeddings of dimension 300 trained on the Spanish Billion Words Corpus. \n",
    "These embeddings were trained using word2vec. --> https://www.kaggle.com/rtatman/pretrained-word-vectors-for-spanish\n",
    "'''\n",
    "\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(dataset_name, encoding=\"utf8\")\n",
    "\n",
    "for a, line in enumerate(f):\n",
    "    if a > 0:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "print(type(embeddings_index))\n",
    "print(len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.96480e-02,  1.13360e-02,  1.99490e-02, -8.88320e-02,\n",
       "       -2.52250e-02,  5.68440e-02,  2.54730e-02,  1.40680e-02,\n",
       "        1.63694e-01, -6.71540e-02,  1.47380e-02,  2.71340e-02,\n",
       "        6.64430e-02, -4.48460e-02, -4.49870e-02, -4.08980e-02,\n",
       "        3.03110e-02,  3.41960e-02, -4.92400e-02,  8.53700e-03,\n",
       "       -6.80910e-02, -8.79380e-02,  3.53000e-02,  1.49385e-01,\n",
       "       -1.23500e-02,  1.26130e-02,  2.93500e-02,  6.95960e-02,\n",
       "        3.91110e-02,  5.76520e-02,  6.99540e-02, -6.62170e-02,\n",
       "       -4.17840e-02,  2.86230e-02,  2.67720e-02, -6.63920e-02,\n",
       "        2.95300e-03, -1.21880e-02, -3.03630e-02,  4.02220e-02,\n",
       "        3.48580e-02,  2.74690e-02, -2.90340e-02, -4.87480e-02,\n",
       "       -3.85820e-02, -5.15530e-02, -3.35010e-02, -1.90080e-02,\n",
       "        3.04300e-03,  1.10712e-01, -2.50960e-02,  1.11082e-01,\n",
       "        3.52440e-02,  1.14207e-01,  1.01950e-02,  5.15110e-02,\n",
       "       -4.06490e-02, -1.13944e-01,  4.48730e-02,  5.20110e-02,\n",
       "        6.73600e-02,  4.90540e-02, -1.27085e-01, -3.18460e-02,\n",
       "        3.28480e-02,  4.08250e-02, -8.48730e-02,  5.98010e-02,\n",
       "       -6.74240e-02,  1.65310e-02, -8.45650e-02,  5.70240e-02,\n",
       "        8.32880e-02, -1.01360e-02, -4.85080e-02,  5.17570e-02,\n",
       "        4.66640e-02,  1.81020e-02, -5.23200e-02, -7.65000e-04,\n",
       "        5.36620e-02, -9.96700e-03,  8.28580e-02,  9.06800e-03,\n",
       "        5.45750e-02, -3.46600e-03, -2.33760e-02,  2.30690e-02,\n",
       "        8.85130e-02,  1.85040e-02, -3.95030e-02, -3.29800e-02,\n",
       "       -2.13900e-03,  1.00000e-05, -1.07627e-01,  7.69900e-03,\n",
       "        4.63510e-02, -3.06200e-03,  3.05000e-02,  1.13650e-01,\n",
       "        3.25360e-02, -9.73010e-02, -1.37340e-02,  9.83450e-02,\n",
       "        8.08980e-02, -6.41730e-02, -8.87400e-03, -1.44751e-01,\n",
       "        3.75850e-02,  1.32900e-02,  5.96740e-02,  6.16300e-03,\n",
       "        7.31800e-03,  5.30000e-05, -6.02920e-02, -5.91350e-02,\n",
       "        4.94970e-02, -1.14380e-02, -9.51080e-02, -4.34650e-02,\n",
       "        4.85670e-02, -4.39900e-02, -3.07740e-02,  5.09200e-03,\n",
       "       -3.22650e-02,  9.39200e-03,  1.85030e-02,  8.48570e-02,\n",
       "        1.09709e-01, -2.06620e-02,  1.76960e-02,  2.66990e-02,\n",
       "       -7.66380e-02, -1.41060e-02, -3.51550e-02,  4.69990e-02,\n",
       "       -3.72700e-03, -4.78050e-02,  4.42700e-02,  1.13140e-02,\n",
       "        3.65240e-02, -6.95050e-02, -1.48500e-02, -3.53800e-03,\n",
       "       -4.70490e-02,  2.93490e-02,  3.45210e-02, -3.21990e-02,\n",
       "        1.16497e-01, -7.76100e-02,  6.82340e-02, -1.61260e-02,\n",
       "       -6.64540e-02, -7.99140e-02, -2.07230e-02, -6.49050e-02,\n",
       "        6.95600e-02,  2.13680e-02, -4.94970e-02, -4.65990e-02,\n",
       "        6.76630e-02, -6.90350e-02,  1.18015e-01,  2.74630e-02,\n",
       "       -6.17600e-03, -3.45140e-02, -2.65150e-02,  4.03080e-02,\n",
       "        9.11130e-02, -8.05390e-02,  1.32408e-01, -7.09580e-02,\n",
       "        1.97300e-02,  3.33990e-02,  3.48900e-03, -1.59659e-01,\n",
       "       -4.23000e-03,  4.88800e-03, -5.66150e-02,  6.10210e-02,\n",
       "        2.51170e-02,  9.96130e-02,  6.38760e-02, -6.20200e-03,\n",
       "       -4.93160e-02, -2.05300e-02,  8.52200e-03, -9.41680e-02,\n",
       "       -9.45100e-03, -3.46820e-02, -2.68010e-02,  6.53830e-02,\n",
       "        4.25280e-02, -1.36880e-02,  3.50680e-02,  1.19803e-01,\n",
       "       -2.02780e-02,  1.11882e-01, -6.83360e-02,  5.02450e-02,\n",
       "       -1.23240e-01,  2.24800e-03,  1.02290e-02, -6.25400e-02,\n",
       "       -1.78370e-02, -1.15210e-01,  5.10360e-02,  2.69200e-02,\n",
       "        8.34570e-02,  6.29020e-02,  3.08300e-03, -3.71120e-02,\n",
       "       -1.54250e-02,  1.71600e-03,  1.80020e-02,  1.01100e-01,\n",
       "        1.94460e-02,  7.48390e-02,  5.99510e-02,  7.22430e-02,\n",
       "       -2.54590e-02, -3.98530e-02,  7.79500e-02,  6.01990e-02,\n",
       "       -7.06030e-02,  6.06520e-02,  2.38550e-02, -3.58580e-02,\n",
       "       -5.80430e-02, -7.51300e-02,  6.70000e-04,  8.28280e-02,\n",
       "       -1.31410e-02,  1.44692e-01, -1.05775e-01,  8.76240e-02,\n",
       "        4.70300e-02, -1.57000e-02, -4.23600e-02,  2.63370e-02,\n",
       "        1.44966e-01,  2.19220e-02,  1.20970e-02, -1.14430e-02,\n",
       "       -1.10208e-01,  9.33300e-03, -8.14230e-02, -2.71370e-02,\n",
       "        8.27000e-04,  8.53890e-02,  3.44640e-02,  3.23380e-02,\n",
       "       -1.50400e-02,  9.95400e-03, -1.17590e-02, -4.23950e-02,\n",
       "       -4.67000e-04,  5.50470e-02, -4.97100e-02, -1.04978e-01,\n",
       "        3.17420e-02,  3.70200e-02, -7.01600e-03, -4.97120e-02,\n",
       "        4.16840e-02,  1.83480e-02, -1.20100e-03,  1.92820e-02,\n",
       "        5.76300e-03,  7.49520e-02, -1.85880e-02,  1.60550e-02,\n",
       "        1.19526e-01, -1.46130e-02,  2.05980e-02,  2.73120e-02,\n",
       "        2.42520e-02, -2.40440e-02, -2.63320e-02, -6.31520e-02,\n",
       "       -9.53630e-02, -3.43350e-02, -6.25520e-02, -3.57300e-02,\n",
       "        9.11650e-02,  9.68600e-03,  2.03410e-02, -1.20040e-02,\n",
       "        1.18260e-02, -8.43010e-02,  1.11440e-02,  7.49690e-02,\n",
       "       -4.86200e-03, -1.40760e-02,  2.56920e-02, -7.73220e-02,\n",
       "       -2.29980e-02, -1.28057e-01, -4.91700e-03,  6.26280e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['de']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ryt5h9p4uZKa"
   },
   "source": [
    "En esta parte se hace el emparejamiento de las 24500 palabras únicas de ancora con las  1000653 de palabras  vectorizadas de SBWE para generar la matriz de embedding con las 24500 palabras  de dimensión 300 como fue preentrenado el SBWE con word2vec. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pNfoH41HuZKb",
    "outputId": "5b1b4f12-f826-4ee3-ac05-c4903c38e90b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24500, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word2index) + 1, 300))\n",
    "\n",
    "for t, (word, i) in enumerate(word2index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # las palabras que no se encuentren en el índice de inserción serán todos ceros.\n",
    "        embedding_matrix[t] = embedding_vector\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.006775  , -0.025222  ,  0.020576  , -0.011762  ,  0.024277  ,\n",
       "       -0.008522  ,  0.04322   ,  0.100305  ,  0.07608   , -0.016098  ,\n",
       "        0.089174  ,  0.004786  , -0.017735  , -0.043213  ,  0.078196  ,\n",
       "        0.050682  ,  0.001926  , -0.073354  , -0.079147  ,  0.067264  ,\n",
       "       -0.007499  , -0.069053  ,  0.054214  , -0.042125  , -0.043433  ,\n",
       "        0.024159  ,  0.024187  ,  0.004847  ,  0.006408  , -0.017422  ,\n",
       "       -0.097593  , -0.047184  ,  0.001357  , -0.088858  ,  0.078469  ,\n",
       "        0.035833  , -0.033951  ,  0.010251  , -0.003062  , -0.010351  ,\n",
       "        0.110404  ,  0.034192  ,  0.017915  , -0.081425  ,  0.100075  ,\n",
       "        0.049019  ,  0.045064  ,  0.061185  , -0.046307  ,  0.151301  ,\n",
       "        0.020501  ,  0.076487  ,  0.038372  , -0.025478  ,  0.065771  ,\n",
       "       -0.017817  , -0.028707  , -0.050361  ,  0.010804  , -0.007707  ,\n",
       "        0.02327   , -0.073957  , -0.026194  , -0.054872  , -0.064792  ,\n",
       "       -0.022397  , -0.002672  ,  0.060082  , -0.020016  ,  0.037563  ,\n",
       "        0.014346  ,  0.040678  , -0.089002  , -0.030903  ,  0.000854  ,\n",
       "        0.09119   , -0.07595   , -0.011915  , -0.099037  ,  0.07847   ,\n",
       "       -0.06239   ,  0.01703   ,  0.033897  ,  0.022007  , -0.006062  ,\n",
       "        0.030159  , -0.021101  ,  0.079626  ,  0.056104  ,  0.008654  ,\n",
       "       -0.023315  ,  0.003611  , -0.012413  ,  0.02588   , -0.068082  ,\n",
       "        0.035461  , -0.109758  ,  0.135236  ,  0.007869  ,  0.019019  ,\n",
       "       -0.045732  , -0.066529  , -0.121203  ,  0.094597  ,  0.005673  ,\n",
       "       -0.047822  ,  0.023767  , -0.099002  ,  0.021782  ,  0.047542  ,\n",
       "        0.049695  ,  0.023992  ,  0.041408  ,  0.013392  , -0.106082  ,\n",
       "       -0.1052    ,  0.113249  , -0.100693  , -0.098304  , -0.191631  ,\n",
       "       -0.084328  , -0.182181  , -0.055088  ,  0.047954  ,  0.041992  ,\n",
       "       -0.010497  ,  0.001301  , -0.020065  ,  0.023609  , -0.075889  ,\n",
       "        0.143482  ,  0.022052  , -0.098143  , -0.026122  , -0.126087  ,\n",
       "       -0.060871  , -0.089241  , -0.091039  ,  0.073572  , -0.073456  ,\n",
       "        0.020031  ,  0.047023  , -0.01889   , -0.058017  , -0.073235  ,\n",
       "       -0.001273  ,  0.038617  , -0.12750401,  0.10095   , -0.048854  ,\n",
       "        0.01094   ,  0.039732  , -0.013276  , -0.015788  , -0.015882  ,\n",
       "        0.07191   ,  0.109005  ,  0.071391  , -0.048421  , -0.02221   ,\n",
       "        0.004153  ,  0.030477  , -0.015307  ,  0.014618  , -0.04886   ,\n",
       "        0.066587  , -0.031288  ,  0.096901  ,  0.03669   , -0.072938  ,\n",
       "       -0.005336  , -0.051173  ,  0.107749  , -0.014661  , -0.019903  ,\n",
       "       -0.056894  ,  0.01268   , -0.100422  ,  0.044429  , -0.063781  ,\n",
       "        0.035401  ,  0.042606  , -0.027791  , -0.048218  , -0.003533  ,\n",
       "        0.050768  , -0.01818   , -0.053154  ,  0.042464  ,  0.01027   ,\n",
       "        0.034385  ,  0.101439  ,  0.01204   ,  0.039714  , -0.041567  ,\n",
       "        0.15542901, -0.003884  , -0.002953  , -0.014531  , -0.048373  ,\n",
       "        0.017149  , -0.004999  ,  0.000904  , -0.001306  , -0.050073  ,\n",
       "       -0.003041  , -0.029823  , -0.055764  ,  0.056085  , -0.028654  ,\n",
       "       -0.043732  ,  0.008044  ,  0.074101  , -0.067782  ,  0.00469   ,\n",
       "       -0.003196  , -0.008206  ,  0.014793  , -0.067418  , -0.02487   ,\n",
       "        0.04646   ,  0.020977  , -0.036689  , -0.006557  , -0.025648  ,\n",
       "        0.05571   ,  0.038885  , -0.006009  , -0.096878  ,  0.005355  ,\n",
       "       -0.002353  , -0.020855  , -0.050248  ,  0.001294  , -0.045283  ,\n",
       "       -0.007155  , -0.062244  , -0.069349  , -0.068386  ,  0.051479  ,\n",
       "        0.061407  , -0.033267  , -0.013084  , -0.066737  , -0.064502  ,\n",
       "       -0.048404  , -0.093919  ,  0.071309  , -0.048597  , -0.057802  ,\n",
       "       -0.038795  , -0.016511  , -0.011406  ,  0.016445  , -0.023537  ,\n",
       "        0.053481  , -0.048001  , -0.049122  , -0.00086   ,  0.021676  ,\n",
       "       -0.017447  ,  0.053422  , -0.004109  , -0.0212    , -0.00312   ,\n",
       "        0.007235  ,  0.092354  ,  0.050126  , -0.009728  ,  0.042875  ,\n",
       "        0.016487  ,  0.084513  ,  0.085093  , -0.007367  ,  0.007796  ,\n",
       "       -0.025832  , -0.026895  ,  0.014824  ,  0.02199   ,  0.05748   ,\n",
       "       -0.081364  ,  0.005571  , -0.061116  ,  0.049984  ,  0.013102  ,\n",
       "       -0.035715  ,  0.052649  ,  0.008894  ,  0.114135  , -0.068298  ,\n",
       "        0.001691  ,  0.084581  , -0.000637  ,  0.070037  , -0.161559  ,\n",
       "       -0.090334  ,  0.004172  , -0.085163  ,  0.090289  , -0.032287  ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g3-1Dd9WuZKj"
   },
   "source": [
    "# Part 2 - Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "_jyfgUOkuZKm",
    "outputId": "c84fc888-1700-4a01-9817-e47aa02a46b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Funcion que permite forzar el uso de GPU cuando estan presentes\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3aS1YGzAuZKt",
    "outputId": "b18e9bcb-801d-41ef-bf94-8e88a2e149b6"
   },
   "outputs": [],
   "source": [
    "!pip3 install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "khAYyR2U4bqm",
    "outputId": "4981ac11-531f-4e61-c66a-4a9f558fcd29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "_-N9qQ8q5hxF",
    "outputId": "7d378141-db6e-4309-93ab-21eeaee9314e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://www.github.com/keras-team/keras-contrib.git"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git 'C:\\Users\\jempool\\AppData\\Local\\Temp\\pip-req-build-h3xd3dko'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Cloning https://www.github.com/keras-team/keras-contrib.git to c:\\users\\jempool\\appdata\\local\\temp\\pip-req-build-h3xd3dko\n",
      "Requirement already satisfied (use --upgrade to upgrade): keras-contrib==2.0.8 from git+https://www.github.com/keras-team/keras-contrib.git in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\n",
      "Requirement already satisfied: keras in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from keras-contrib==2.0.8) (2.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from keras->keras-contrib==2.0.8) (1.18.5)\n",
      "Requirement already satisfied: h5py in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from keras->keras-contrib==2.0.8) (5.3.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
      "Building wheels for collected packages: keras-contrib\n",
      "  Building wheel for keras-contrib (setup.py): started\n",
      "  Building wheel for keras-contrib (setup.py): finished with status 'done'\n",
      "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101658 sha256=eb8513ed92c111fe85158870dd3b9e8dadf56641dd360d03058bf9f7aa1d201e\n",
      "  Stored in directory: C:\\Users\\jempool\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-5xu2ctqc\\wheels\\16\\87\\6e\\8e3b73f23fb38163af1c319aa23f14602018b501ecb91430a2\n",
      "Successfully built keras-contrib\n"
     ]
    }
   ],
   "source": [
    "!pip3 install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "ELLI2jA5uZKy",
    "outputId": "b2042bb5-d719-477b-b90e-532311b1d985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 149)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 149, 300)          7349700   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 149, 600)          1442400   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 149, 600)          2882400   \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 149, 291)          174891    \n",
      "_________________________________________________________________\n",
      "crf_4 (CRF)                  (None, 149, 291)          170235    \n",
      "=================================================================\n",
      "Total params: 12,019,626\n",
      "Trainable params: 12,019,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "import keras as k\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "input = Input(shape=(MAX_LENGTH,))\n",
    "word_embedding_size = 300\n",
    "\n",
    "# Embedding Layer\n",
    "model = Embedding(input_dim=len(word2index), output_dim=word_embedding_size, input_length=MAX_LENGTH)(input)\n",
    "\n",
    "# BI-LSTM Layer\n",
    "model = Bidirectional(LSTM(units=word_embedding_size, \n",
    "                           return_sequences=True, \n",
    "                           dropout=0.5, \n",
    "                           recurrent_dropout=0.5, \n",
    "                           kernel_initializer=k.initializers.he_normal()))(model)\n",
    "\n",
    "model = LSTM(units=word_embedding_size * 2, \n",
    "             return_sequences=True, \n",
    "             dropout=0.5, \n",
    "             recurrent_dropout=0.5, \n",
    "             kernel_initializer=k.initializers.he_normal())(model)\n",
    "\n",
    "# TimeDistributed Layer\n",
    "model = TimeDistributed(Dense(len(tag2index), activation=\"relu\"))(model)  \n",
    "\n",
    "# CRF Layer\n",
    "crf = CRF(len(tag2index))\n",
    "\n",
    "out = crf(model)  # output\n",
    "model = Model(input, out)\n",
    "\n",
    "\n",
    "#Optimiser \n",
    "adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "colab_type": "code",
    "id": "fG1ufNkwuZK4",
    "outputId": "32ea814f-db4f-4894-a840-335ee9779b3f"
   },
   "outputs": [],
   "source": [
    "#sudo pip install h5py\n",
    "import os\n",
    "model_hist = model.fit(train_sentences_X, cat_train_tags_y,\n",
    "                       validation_data=(eval_sentences_X, cat_eval_tags_y),\n",
    "                       batch_size=128, \n",
    "                       epochs=20,\n",
    "                       validation_split=0.1, verbose=1)\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"mb-full.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"mb-full.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown loss function:crf_loss",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-7544973bd2e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcustom_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mb-full-model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'CRF'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCRF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'loss'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#model = model_from_json(open(\"mb-full.json\").read())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_supported_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh5dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'write'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mload_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh5file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36m_deserialize_model\u001b[1;34m(h5dict, custom_objects, compile)\u001b[0m\n\u001b[0;32m    367\u001b[0m                       \u001b[0mweighted_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweighted_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m                       \u001b[0mloss_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m                       sample_weight_mode=sample_weight_mode)\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[1;31m# Set optimizer weights.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# Prepare list of loss functions, same size as model outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         self.loss_functions = training_utils.prepare_loss_functions(\n\u001b[1;32m--> 119\u001b[1;33m             self.loss, self.output_names)\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mprepare_loss_functions\u001b[1;34m(loss, output_names)\u001b[0m\n\u001b[0;32m    820\u001b[0m             \u001b[0mloss_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_loss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m         \u001b[0mloss_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_loss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    820\u001b[0m             \u001b[0mloss_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_loss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m         \u001b[0mloss_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_loss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mget_loss_function\u001b[1;34m(loss)\u001b[0m\n\u001b[0;32m    703\u001b[0m     \u001b[1;31m# Wrap loss function with signature `(y_true, y_pred, **kwargs)`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[1;31m# in `LossFunctionWrapper` class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 705\u001b[1;33m     \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m     \u001b[1;31m# For losses which are given as strings/functions in the compile API,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m         \u001b[0midentifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    796\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(name, custom_objects)\u001b[0m\n\u001b[0;32m    774\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 776\u001b[1;33m                                     printable_module_name='loss function')\n\u001b[0m\u001b[0;32m    777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[1;32m--> 167\u001b[1;33m                                  ':' + function_name)\n\u001b[0m\u001b[0;32m    168\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown loss function:crf_loss"
     ]
    }
   ],
   "source": [
    "loss = crf.loss_function\n",
    "import keras.losses\n",
    "keras.losses.custom_loss = loss\n",
    "\n",
    "model = load_model(\"mb-full-model\", custom_objects={'CRF': CRF, 'loss': loss })\n",
    "\n",
    "#model = model_from_json(open(\"mb-full.json\").read())\n",
    "#model.load_weights(os.path.join(os.path.dirname(\"mb-full.h5\"), 'model_weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------------\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "import json\n",
    "'''\n",
    "with open('mb-full.json','r') as f:\n",
    "    model_json = json.load(f)\n",
    "\n",
    "model = model_from_json(model_json) '''\n",
    "model.load_weights(\"mb-full.h5\")\n",
    "#------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCfeub5auZK8"
   },
   "source": [
    "# Parte 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "GFzKxQtYuZK_",
    "outputId": "f75b3573-285d-44c5-d091-cc7d15997712"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 640/1206 [==============>...............] - ETA: 3:19"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-d3291d196bd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_sentences_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_test_tags_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# acc: 97.66269326210022\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1359\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m                                          \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m                                          callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m     def predict(self, x,\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3792\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3794\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m     \"\"\"\n\u001b[1;32m-> 1605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1645\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_sentences_X, cat_test_tags_y)\n",
    "print(scores[1] * 100)   # acc: 97.66269326210022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Qbe0SmiuZLG"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_model_performance(train_loss, train_acc, train_val_loss, train_val_acc):\n",
    "    \"\"\" Plot model loss and accuracy through epochs. \"\"\"\n",
    "    blue= '#34495E'\n",
    "    green = '#2ECC71'\n",
    "    orange = '#E23B13'\n",
    "    \n",
    "    # plot model loss\n",
    "    fig, (ax1, ax2) = plt.subplots(2, figsize=(10, 8))\n",
    "    ax1.plot(range(1, len(train_loss) + 1), train_loss, blue, linewidth=5, label='training')\n",
    "    ax1.plot(range(1, len(train_val_loss) + 1), train_val_loss, green, linewidth=5, label='validation')\n",
    "    ax1.set_xlabel('# epoch')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.tick_params('y')\n",
    "    ax1.legend(loc='upper right', shadow=False)\n",
    "    ax1.set_title('Model loss through #epochs', color=orange, fontweight='bold')\n",
    "    \n",
    "    # plot model accuracy\n",
    "    ax2.plot(range(1, len(train_acc) + 1), train_acc, blue, linewidth=5, label='training')\n",
    "    ax2.plot(range(1, len(train_val_acc) + 1), train_val_acc, green, linewidth=5, label='validation')\n",
    "    ax2.set_xlabel('# epoch')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    ax2.tick_params('y')\n",
    "    ax2.legend(loc='lower right', shadow=False)\n",
    "    ax2.set_title('Model accuracy through #epochs', color=orange, fontweight='bold')\n",
    "    \n",
    "    # fig.savefig('Plot/training/training-mb-13.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "zUYIXicVuZLK",
    "outputId": "e83b899b-d76c-450f-96a5-99ce07d7a0b2"
   },
   "outputs": [],
   "source": [
    "plot_model_performance(\n",
    "    train_loss=model_hist.history.get('loss', []),\n",
    "    train_acc=model_hist.history.get('crf_viterbi_accuracy', []),\n",
    "    train_val_loss=model_hist.history.get('val_loss', []),\n",
    "    train_val_acc=model_hist.history.get('val_crf_viterbi_accuracy', [])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ekKGpLjuZLQ"
   },
   "source": [
    "## Función que Permite convertir Indices en Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1yWdiaCduZLR"
   },
   "outputs": [],
   "source": [
    "#ESTA FUNCION RECIBE EN sequences LA LISTA DE ORACIONES DONDE CADA ELEMENTO DE LA ORACION ES UN ONE HOT VECTOR\n",
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-H9XY_HtuZLW"
   },
   "source": [
    "## Hacemos la prediccion sobre el conjunto de pruebas. De la distribución probabilítica a etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "oViREREmuZLW",
    "outputId": "449e5f95-bf13-4dac-be81-b0b4e81a89d3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "prediction = model.predict(test_sentences_X)\n",
    "log_tokens = logits_to_tokens(prediction, {i: t for t, i in tag2index.items()})\n",
    "\n",
    "print(log_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zSE-XrafuZLa",
    "outputId": "efc4bc40-2199-4fde-863d-5bf37e4088e1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "results = pd.DataFrame(columns=['Expected', 'Predicted'])\n",
    "k = 0\n",
    "for i, lista_etiquetas_oracion in enumerate(test_tags):\n",
    "    for j, etiquetas in enumerate(lista_etiquetas_oracion):\n",
    "        k = k + 1\n",
    "        results.loc[k, 'Expected'] = etiquetas\n",
    "        results.loc[k, 'Predicted'] = log_tokens[i][j]\n",
    "\n",
    "print(results)\n",
    "\n",
    "\n",
    "print('\\nclassification_report:\\n', classification_report(results['Expected'], results['Predicted']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LswNKDP6uZLf"
   },
   "source": [
    "## PARTE 4 - Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yqFPE0f6uZLi"
   },
   "source": [
    "### Creamos un pequeño Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ZWoNvq1PuZLj",
    "outputId": "8e95ccef-bd17-401b-f62b-549e631d1e70"
   },
   "outputs": [],
   "source": [
    "test_samples = [\n",
    "    \"Correr es importante para mi .\".split(),\n",
    "    \"El hombre bajo corre bajo el puente con bajo índice de adrenalina .\".split()\n",
    "]\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apvItfYmuZLo"
   },
   "source": [
    "### Convertimos el texto en Una entrada para el Modelo y se generan los dos vecores de enteros de las dos oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "sTZYQ_QxuZLp",
    "outputId": "134e3b7d-692e-4c5d-d05d-54015162088f"
   },
   "outputs": [],
   "source": [
    "test_samples_X = []\n",
    "for s in test_samples:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    "    test_samples_X.append(s_int)\n",
    "\n",
    "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
    "print(test_samples_X)\n",
    "print(test_samples_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UZQN_OFwuZLx"
   },
   "source": [
    "### Se Ejecuta la predicion con la Entrada del modelo entrenado y el modelo de la red neuronal predice un matriz de 149 X 291 por cada oración. El shape de a predicción es (2, 149,291)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "Pp1jIdOkuZLy",
    "outputId": "4c374ca3-ade2-4217-ee16-b1f11be60fcb"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_samples_X)\n",
    "print(predictions, predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ur22hbYFuZL2"
   },
   "source": [
    "### Conversion de la Salida del Modelo a un lista de Indices de Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "p0WdcRY5uZL2",
    "outputId": "d87444e7-5727-4731-fc87-19b388922af2"
   },
   "outputs": [],
   "source": [
    "log_tokens = logits_to_tokens(predictions, {i: t for t, i in tag2index.items()})\n",
    "# JUS FOR CHECKING!!!! - IGNORE IT\n",
    "ll = {i: t for t, i in tag2index.items()}\n",
    "print(ll[np.argmax(predictions[0][0])])\n",
    "####################\n",
    "print(log_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p77ajZAPuZL7"
   },
   "source": [
    "### Presentación de los Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "xml5knm6uZL8",
    "outputId": "bad2d88f-5034-4562-fafb-2b4ecb6bcf23"
   },
   "outputs": [],
   "source": [
    "#!pip install tabulate\n",
    "from tabulate import tabulate\n",
    "\n",
    "heads1 = test_samples[0]\n",
    "body1 = [log_tokens[0][:len(test_samples[0])]]\n",
    "\n",
    "heads2 = test_samples[1]\n",
    "body2 = [log_tokens[1][:len(test_samples[1])]]\n",
    "\n",
    "print(tabulate(body1, headers=heads1))\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "print(goodPrint(heads2))\n",
    "print(body2)\n",
    "\n",
    "\n",
    "## postagging Freeling 4.1\n",
    "\n",
    "## El      hombre   bajo     corre    bajo  el      puente   con  bajo  índice   de  adrenalina  .\n",
    "## DA0MS0  NCMS000  AQ0MS00  VMIP3S0  SP    DA0MS0  NCMS000  SP   SP    NCMS000  SP  NCFS000     Fp\n",
    "\n",
    "\n",
    "## pos tagger Stanford NLP\n",
    "\n",
    "## El      hombre   bajo     corre    bajo  el      puente   con    bajo   índice  de    adrenalina  .\n",
    "## da0000  nc0s000  aq0000   vmip000  sp000 da0000  nc0s000  sp000  aq0000 nc0s000 sp000 nc0s000     fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lIsLOwE3uZL_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Pos_tagging2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
