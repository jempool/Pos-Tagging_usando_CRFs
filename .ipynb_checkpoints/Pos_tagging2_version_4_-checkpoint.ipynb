{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMRragjvkU4J"
   },
   "source": [
    "## Python Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "XcHO9PljuZHw",
    "outputId": "a2e12df9-a3c9-4c83-cdc9-7f6ec4278218"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git 'C:\\Users\\jempool\\AppData\\Local\\Temp\\pip-req-build-_hy09for'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
      "  Cloning https://www.github.com/keras-team/keras-contrib.git to c:\\users\\jempool\\appdata\\local\\temp\\pip-req-build-_hy09for\n",
      "Requirement already satisfied (use --upgrade to upgrade): keras-contrib==2.0.8 from git+https://www.github.com/keras-team/keras-contrib.git in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages\n",
      "Requirement already satisfied: keras in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from keras-contrib==2.0.8) (2.3.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from keras->keras-contrib==2.0.8) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from keras->keras-contrib==2.0.8) (1.18.5)\n",
      "Requirement already satisfied: h5py in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
      "Building wheels for collected packages: keras-contrib\n",
      "  Building wheel for keras-contrib (setup.py): started\n",
      "  Building wheel for keras-contrib (setup.py): finished with status 'done'\n",
      "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101658 sha256=25776bd37be401768badda742be7fdf48733f5d2ca5653cf3eb2b9c41e4c58f9\n",
      "  Stored in directory: C:\\Users\\jempool\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-wgxg_m19\\wheels\\16\\87\\6e\\8e3b73f23fb38163af1c319aa23f14602018b501ecb91430a2\n",
      "Successfully built keras-contrib\n",
      "Requirement already satisfied: seqeval in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (0.0.12)\n",
      "Requirement already satisfied: Keras>=2.2.4 in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from seqeval) (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from seqeval) (1.18.5)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from Keras>=2.2.4->seqeval) (1.1.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from Keras>=2.2.4->seqeval) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from Keras>=2.2.4->seqeval) (1.15.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\jempool\\desktop\\introd~1\\virtua~1\\lib\\site-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras as k\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Input, load_model\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "from keras_contrib.layers import CRF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "!pip install seqeval\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from tkinter import *\n",
    "from tkinter.ttk import *\n",
    "from prettytable import PrettyTable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jnO94rlfuZIL"
   },
   "source": [
    "## Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "9ahWEusT1Zy1",
    "outputId": "8ce728c6-f09d-48ea-9bed-62c7ebedae14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "097oMpQ2kU4j"
   },
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "6HLjgn9ikU4l",
    "outputId": "e35f840d-3d41-4632-aecf-fe7c51195376"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cess_esp to\n",
      "[nltk_data]     C:\\Users\\jempool\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cess_esp is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [('El', 'da0ms0'), ('grupo', 'ncms000'), ('estatal', 'aq0cs0'), ('Electricité_de_France', 'np00000'), ('-Fpa-', 'Fpa'), ('EDF', 'np00000'), ('-Fpt-', 'Fpt'), ('anunció', 'vmis3s0'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da0fs0'), ('compra', 'ncfs000'), ('del', 'spcms'), ('51_por_ciento', 'Zp'), ('de', 'sps00'), ('la', 'da0fs0'), ('empresa', 'ncfs000'), ('mexicana', 'aq0fs0'), ('Electricidad_Águila_de_Altamira', 'np00000'), ('-Fpa-', 'Fpa'), ('EAA', 'np00000'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq0fsp'), ('por', 'sps00'), ('el', 'da0ms0'), ('japonés', 'aq0ms0'), ('Mitsubishi_Corporation', 'np00000'), ('para', 'sps00'), ('poner_en_marcha', 'vmn0000'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('de', 'sps00'), ('495', 'Z'), ('megavatios', 'ncmp000'), ('.', 'Fp')] \n",
      "\n",
      "Tagged sentences:  6030\n",
      "Tagged words: 192686\n"
     ]
    }
   ],
   "source": [
    "nltk.download('cess_esp')\n",
    "from nltk.corpus import cess_esp\n",
    "\n",
    "tagged_sentences = cess_esp.tagged_sents()\n",
    "\n",
    "print(\"\\n\", tagged_sentences[0], \"\\n\")\n",
    "print(\"Tagged sentences: \", len(tagged_sentences))\n",
    "print(\"Tagged words:\", len(cess_esp.tagged_words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S7qhFY6quZIz"
   },
   "source": [
    "## Sample of the arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "IhQ_o1RruZI1",
    "outputId": "cfa8530f-5f3c-48d4-f54a-fc74bed7b468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6030 sentences\n",
      "\n",
      "40 words in the first sentence\n",
      "\n",
      "El grupo estatal Electricité_de_France -Fpa- EDF -Fpt- anunció hoy , jueves , la compra del 51_por_ciento de la empresa mexicana Electricidad_Águila_de_Altamira -Fpa- EAA -Fpt- , creada por el japonés Mitsubishi_Corporation para poner_en_marcha una central de gas de 495 megavatios . \n",
      "\n",
      "40 tags in the first sentence\n",
      "\n",
      "['da0ms0' 'ncms000' 'aq0cs0' 'np00000' 'Fpa' 'np00000' 'Fpt' 'vmis3s0'\n",
      " 'rg' 'Fc' 'W' 'Fc' 'da0fs0' 'ncfs000' 'spcms' 'Zp' 'sps00' 'da0fs0'\n",
      " 'ncfs000' 'aq0fs0' 'np00000' 'Fpa' 'np00000' 'Fpt' 'Fc' 'aq0fsp' 'sps00'\n",
      " 'da0ms0' 'aq0ms0' 'np00000' 'sps00' 'vmn0000' 'di0fs0' 'ncfs000' 'sps00'\n",
      " 'ncms000' 'sps00' 'Z' 'ncmp000' 'Fp']\n"
     ]
    }
   ],
   "source": [
    "# Split sentences and tags into two different arrays\n",
    "sentences, tagss = [], []\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    tagss.append(np.array(tags))\n",
    "    sentences.append(np.array(sentence))\n",
    "\n",
    "# Custom function for print a sentence\n",
    "def goodPrint(a):\n",
    "    x = \"\"\n",
    "    for i in range(len(a)):\n",
    "        x += a[i]+\" \"\n",
    "    return x\n",
    "\n",
    "print(str(len(sentences)) + \" sentences\\n\")\n",
    "print(str(len(sentences[0])) + \" words in the first sentence\\n\")\n",
    "print(goodPrint(sentences[0])+ \"\\n\")\n",
    "print(str(len(tagss[0])) +\" tags in the first sentence\\n\" )\n",
    "print(tagss[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ACHtdEPRuZI6"
   },
   "source": [
    "## Percentages for training, testing and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "id": "7OizRh02uZI8",
    "outputId": "00332e9f-c31a-4d1d-ac0c-379000840d10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "3618  sentences for training (60%)\n",
      "1206  sentences for testing (20%)\n",
      "1206  sentences for evaluating (20%)\n",
      "\n",
      "first sentence for training ( 39  words )\n",
      " ['La' 'firma' 'asociada' 'de' 'Mission' ',' 'Azteca_Milling' ',' 'que'\n",
      " 'es' 'la' 'proveedora' 'de' 'la' 'harina' 'para' 'los' 'tacos' 'y'\n",
      " 'tortillas' 'de' 'Mission_Foods' 'y' 'Kraft' ',' 'anunció' 'que' '*0*'\n",
      " 'retiraría' 'del' 'mercado' 'toda' 'la' 'harina' 'hecha' 'con' 'maíz'\n",
      " 'amarillo' '.']\n",
      "\n",
      "first sentence for testing ( 11  words )\n",
      " ['El' 'taxista' 'seguía' 'con' 'su' 'exposición' 'tan' 'maravillada'\n",
      " 'como' 'rutinaria' '.']\n",
      "\n",
      "first sentence for evaluating ( 12  words )\n",
      " ['\"' 'La' 'victoria' 'de' 'hoy' 'elevó' 'a' 'Guga' 'a' 'otro' 'nivel' '.']\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "3618  tags for training (60%)\n",
      "1206  tags for testing (20%)\n",
      "1206  tags for evaluating (20%)\n",
      "\n",
      "first tag for training  ( 39  tags )\n",
      " ['da0fs0' 'ncfs000' 'aq0fsp' 'sps00' 'np0000o' 'Fc' 'np0000o' 'Fc'\n",
      " 'pr0cn000' 'vsip3s0' 'da0fs0' 'ncfs000' 'sps00' 'da0fs0' 'ncfs000'\n",
      " 'sps00' 'da0mp0' 'ncmp000' 'cc' 'ncfp000' 'sps00' 'np0000o' 'cc'\n",
      " 'np0000o' 'Fc' 'vmis3s0' 'cs' 'sn.e-SUJ' 'vmic3s0' 'spcms' 'ncms000'\n",
      " 'di0fs0' 'da0fs0' 'ncfs000' 'aq0fsp' 'sps00' 'ncms000' 'aq0ms0' 'Fp']\n",
      "\n",
      "first tag for testing ( 11  tags )\n",
      " ['da0ms0' 'nccs000' 'vmii3s0' 'sps00' 'dp3cs0' 'ncfs000' 'rg' 'aq0fsp'\n",
      " 'cs' 'aq0fs0' 'Fp']\n",
      "\n",
      "first tag for evaluating ( 12  tags )\n",
      " ['Fe' 'da0fs0' 'ncfs000' 'sps00' 'rg' 'vmis3s0' 'sps00' 'np0000p' 'sps00'\n",
      " 'di0ms0' 'ncms000' 'Fp']\n"
     ]
    }
   ],
   "source": [
    "(training_sentences, \n",
    " test_sentences, \n",
    " training_tags, \n",
    " test_tags) = train_test_split(sentences, tagss, test_size=0.2)\n",
    "\n",
    "(train_sentences, \n",
    " eval_sentences, \n",
    " train_tags, \n",
    " eval_tags) = train_test_split(training_sentences, training_tags, test_size=0.25)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(str(len(train_sentences)), \" sentences for training (60%)\")\n",
    "print(str(len(test_sentences)), \" sentences for testing (20%)\")\n",
    "print(str(len(eval_sentences)), \" sentences for evaluating (20%)\\n\")\n",
    "\n",
    "print(\"first sentence for training (\",len(train_sentences[0]) , \" words )\\n\", train_sentences[0])\n",
    "print(\"\\nfirst sentence for testing (\",len(test_sentences[0]) , \" words )\\n\", test_sentences[0])\n",
    "print(\"\\nfirst sentence for evaluating (\",len(eval_sentences[0]) , \" words )\\n\", eval_sentences[0])\n",
    "\n",
    "print(\"\\n---------------------------------------------------------------------------\\n\")\n",
    "\n",
    "print(str(len(train_tags)), \" tags for training (60%)\")\n",
    "print(str(len(test_tags)), \" tags for testing (20%)\")\n",
    "print(str(len(eval_tags)), \" tags for evaluating (20%)\\n\")\n",
    "\n",
    "print(\"first tag for training  (\",len(train_tags[0]) , \" tags )\\n\", train_tags[0])\n",
    "print(\"\\nfirst tag for testing (\",len(test_tags[0]) , \" tags )\\n\", test_tags[0])\n",
    "print(\"\\nfirst tag for evaluating (\",len(eval_tags[0]) , \" tags )\\n\", eval_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHecruQluZJK"
   },
   "source": [
    "## Word and tag dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "jLlvGcXpuZJL",
    "outputId": "93f8bc1c-4b4d-443f-ee36-5040ee752727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word dictionary length:  24499\n",
      "Tag dictionary length:  291\n"
     ]
    }
   ],
   "source": [
    "words, tagsss = set([]), set([])\n",
    " \n",
    "for s in (train_sentences + eval_sentences + test_sentences):\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    "\n",
    "for ts in (train_tags + eval_tags + test_tags):\n",
    "    for t in ts:\n",
    "        tagsss.add(t)\n",
    "\n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 2 for i, t in enumerate(list(tagsss))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding\n",
    "tag2index['-OOV-'] = 1  # The special value used to OOVs\n",
    "\n",
    "print (\"\\nWord dictionary length: \", len(word2index))\n",
    "print (\"Tag dictionary length: \", len(tag2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6qh3so4uZJR"
   },
   "source": [
    "## Parsing Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "id": "YcKOFMa4uZJS",
    "outputId": "7a9efaea-b4d3-4f7d-e55c-a7bac62c502c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "first parsed sentence for training ( 39  words )\n",
      "\n",
      " [1298, 10827, 10988, 7145, 1062, 12402, 5610, 12402, 18505, 506, 1298, 20779, 7145, 1298, 480, 22486, 20299, 8073, 12129, 7368, 7145, 6765, 12129, 11087, 12402, 6834, 18505, 14160, 19827, 933, 6931, 5847, 1298, 480, 7224, 22324, 23655, 21712, 17042]\n",
      "\n",
      "tags of the first parsed sentence for training ( 39  tags )\n",
      "\n",
      " [117, 172, 76, 287, 41, 196, 41, 196, 9, 73, 117, 172, 287, 117, 172, 287, 109, 131, 218, 127, 287, 41, 218, 41, 196, 280, 22, 2, 207, 90, 204, 45, 117, 172, 76, 287, 204, 99, 286]\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "first parsed sentence for testing ( 11  words )\n",
      "\n",
      " [5893, 22699, 17859, 22324, 18972, 14682, 7073, 6113, 15795, 6191, 17042]\n",
      "\n",
      "tags of the first parsed sentence for testing ( 11  tags )\n",
      "\n",
      " [208, 190, 136, 287, 126, 172, 151, 76, 22, 23, 286]\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "first parsed sentence for evaluating ( 12  words )\n",
      "\n",
      " [8071, 1298, 16784, 7145, 17866, 23711, 5233, 12018, 5233, 17972, 20213, 17042]\n",
      "\n",
      "tags of the first parsed sentence for evaluating ( 12  tags )\n",
      "\n",
      " [59, 117, 172, 287, 151, 280, 287, 212, 287, 20, 204, 286]\n"
     ]
    }
   ],
   "source": [
    "train_sentences_X, eval_sentences_X, test_sentences_X, train_tags_y, eval_tags_y, test_tags_y = [], [], [], [], [], []\n",
    "\n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    "\n",
    "for s in eval_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    eval_sentences_X.append(s_int)\n",
    "\n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    "\n",
    "for s in train_tags:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(tag2index[w])\n",
    "        except KeyError:\n",
    "            s_int.append(tag2index['-OOV-'])\n",
    "            \n",
    "    train_tags_y.append(s_int)\n",
    "\n",
    "for s in eval_tags:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(tag2index[w])\n",
    "        except KeyError:\n",
    "            s_int.append(tag2index['-OOV-'])\n",
    "            \n",
    "    eval_tags_y.append(s_int)\n",
    "\n",
    "for s in test_tags:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(tag2index[w])\n",
    "        except KeyError:\n",
    "            s_int.append(tag2index['-OOV-'])\n",
    "            \n",
    "    test_tags_y.append(s_int)\n",
    "\n",
    "\n",
    "print(\"\\n--------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\nfirst parsed sentence for training (\", len(train_sentences_X[0]),\" words )\\n\\n\", train_sentences_X[0])\n",
    "print(\"\\ntags of the first parsed sentence for training (\", len(train_tags_y[0]),\" tags )\\n\\n\", train_tags_y[0])\n",
    "print(\"\\n--------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\nfirst parsed sentence for testing (\", len(test_sentences_X[0]),\" words )\\n\\n\", test_sentences_X[0])\n",
    "print(\"\\ntags of the first parsed sentence for testing (\", len(test_tags_y[0]),\" tags )\\n\\n\", test_tags_y[0])\n",
    "print(\"\\n--------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\nfirst parsed sentence for evaluating (\", len(eval_sentences_X[0]),\" words )\\n\\n\", eval_sentences_X[0])\n",
    "print(\"\\ntags of the first parsed sentence for evaluating (\", len(eval_tags_y[0]),\" tags )\\n\\n\", eval_tags_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mw78VGYxuZJp"
   },
   "source": [
    "## Normalization based on the longest sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HOCcNUJWuZJq",
    "outputId": "88dc1a0b-5743-45a0-8c7e-f974177d79d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Longest sentence in training set ->  149  words\n",
      "Longest sentence in evaluating set ->  131  words\n",
      "Longest sentence in testing set ->  129  words\n",
      "\n",
      "Longest sentence in all sets ->  149  words\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "first parsed and normalized sentence for training\n",
      "\n",
      " [ 1298 10827 10988  7145  1062 12402  5610 12402 18505   506  1298 20779\n",
      "  7145  1298   480 22486 20299  8073 12129  7368  7145  6765 12129 11087\n",
      " 12402  6834 18505 14160 19827   933  6931  5847  1298   480  7224 22324\n",
      " 23655 21712 17042     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0]\n",
      "\n",
      "tags\n",
      "\n",
      " [117 172  76 287  41 196  41 196   9  73 117 172 287 117 172 287 109 131\n",
      " 218 127 287  41 218  41 196 280  22   2 207  90 204  45 117 172  76 287\n",
      " 204  99 286   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0]\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "first parsed and normalized sentence for evaluating\n",
      "\n",
      " [ 8071  1298 16784  7145 17866 23711  5233 12018  5233 17972 20213 17042\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0]\n",
      "\n",
      "tags\n",
      "\n",
      " [ 59 117 172 287 151 280 287 212 287  20 204 286   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0]\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "first parsed and normalized sentence for testing\n",
      "\n",
      " [ 5893 22699 17859 22324 18972 14682  7073  6113 15795  6191 17042     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0]\n",
      "\n",
      "tags\n",
      "\n",
      " [208 190 136 287 126 172 151  76  22  23 286   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH_TRAIN = len(max(train_sentences_X, key=len))\n",
    "MAX_LENGTH_EVAL = len(max(eval_sentences_X, key=len))\n",
    "MAX_LENGTH_TEST = len(max(test_sentences_X, key=len))\n",
    "\n",
    "print(\"\\nLongest sentence in training set -> \", MAX_LENGTH_TRAIN, \" words\")\n",
    "print(\"Longest sentence in evaluating set -> \", MAX_LENGTH_EVAL, \" words\")\n",
    "print(\"Longest sentence in testing set -> \", MAX_LENGTH_TEST, \" words\")\n",
    "\n",
    "MAX_LENGTH = max(MAX_LENGTH_TRAIN, MAX_LENGTH_EVAL, MAX_LENGTH_TEST)\n",
    "print(\"\\nLongest sentence in all sets -> \", MAX_LENGTH, \" words\\n\")\n",
    "print(\"-------------------------------------------------------------------------\\n\")\n",
    "\n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "eval_sentences_X = pad_sequences(eval_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "eval_tags_y = pad_sequences(eval_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "print(\"first parsed and normalized sentence for training\\n\\n\", train_sentences_X[0])\n",
    "print(\"\\ntags\\n\\n\", train_tags_y[0])\n",
    "print(\"\\n-------------------------------------------------------------------------\\n\")\n",
    "print(\"first parsed and normalized sentence for evaluating\\n\\n\", eval_sentences_X[0])\n",
    "print(\"\\ntags\\n\\n\", eval_tags_y[0])\n",
    "print(\"\\n-------------------------------------------------------------------------\\n\")\n",
    "print(\"first parsed and normalized sentence for testing\\n\\n\", test_sentences_X[0])\n",
    "print(\"\\ntags\\n\\n\", test_tags_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sWwZ3UuAuZJ3"
   },
   "source": [
    "## Tags to One-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "id": "pmckYX7tuZJ4",
    "outputId": "ca54f8dd-5e4f-4ed4-f0e9-63b73859a2a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "parsed and normalized tags for the first sentence\n",
      "\n",
      " [117 172  76 287  41 196  41 196   9  73 117 172 287 117 172 287 109 131\n",
      " 218 127 287  41 218  41 196 280  22   2 207  90 204  45 117 172  76 287\n",
      " 204  99 286   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0] \n",
      "\n",
      "one-hot vectors matrix for the first sentence tags\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]] \n",
      "\n",
      "one-hot vector for the first sentence tag\n",
      "\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def to_categoricals(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)\n",
    "\n",
    "cat_train_tags_y = to_categoricals(train_tags_y, len(tag2index))\n",
    "cat_eval_tags_y  = to_categoricals(eval_tags_y, len(tag2index))\n",
    "cat_test_tags_y  = to_categoricals(test_tags_y, len(tag2index))\n",
    "\n",
    "print(\"\\nparsed and normalized tags for the first sentence\\n\\n\", train_tags_y[0], \"\\n\")\n",
    "print(\"one-hot vectors matrix for the first sentence tags\\n\\n\", cat_train_tags_y[0], \"\\n\")\n",
    "print(\"one-hot vector for the first sentence tag\\n\\n\", cat_train_tags_y[0][0], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g3-1Dd9WuZKj"
   },
   "source": [
    "# <font color='green'>Training</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "_jyfgUOkuZKm",
    "outputId": "0cd8407d-18f4-4e16-d69b-6a45906693fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "## Funcion que permite forzar el uso de GPU cuando estan presentes\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "ELLI2jA5uZKy",
    "outputId": "d0f9c733-2601-44e5-93cc-7679489b94f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 149)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 149, 300)          7349700   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 149, 600)          1442400   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 149, 600)          2882400   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 149, 291)          174891    \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 149, 291)          170235    \n",
      "=================================================================\n",
      "Total params: 12,019,626\n",
      "Trainable params: 12,019,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(MAX_LENGTH,))\n",
    "word_embedding_size = 300\n",
    "\n",
    "# Embedding Layer\n",
    "model = Embedding(input_dim=len(word2index), output_dim=word_embedding_size, input_length=MAX_LENGTH)(input)\n",
    "\n",
    "# BI-LSTM Layer\n",
    "model = Bidirectional(LSTM(units=word_embedding_size, \n",
    "                           return_sequences=True, \n",
    "                           dropout=0.5, \n",
    "                           recurrent_dropout=0.5, \n",
    "                           kernel_initializer=k.initializers.he_normal()))(model)\n",
    "\n",
    "model = LSTM(units=word_embedding_size * 2, \n",
    "             return_sequences=True, \n",
    "             dropout=0.5, \n",
    "             recurrent_dropout=0.5, \n",
    "             kernel_initializer=k.initializers.he_normal())(model)\n",
    "\n",
    "# TimeDistributed Layer\n",
    "model = TimeDistributed(Dense(len(tag2index), activation=\"relu\"))(model)  \n",
    "\n",
    "# CRF Layer\n",
    "crf = CRF(len(tag2index))\n",
    "\n",
    "out = crf(model)  # output\n",
    "model = Model(input, out)\n",
    "\n",
    "\n",
    "#Optimiser \n",
    "adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])\n",
    "print(\"\\n\")\n",
    "model.summary()\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='orange'>[!] If using low-end GPU, COMMENT this block</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "fG1ufNkwuZK4",
    "outputId": "851914d6-bd13-4c97-ccc6-d4824c5b7771"
   },
   "outputs": [],
   "source": [
    "# model_hist = model.fit(train_sentences_X, cat_train_tags_y,\n",
    "#                        validation_data=(eval_sentences_X, cat_eval_tags_y),\n",
    "#                        batch_size=64, \n",
    "#                        epochs=25,\n",
    "#                        validation_split=0.1,\n",
    "#                        verbose=1)\n",
    "\n",
    "\n",
    "# # serialize model to JSON\n",
    "# model_json = model.to_json()\n",
    "# with open(\"mb-full.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "\n",
    "# # serialize weights to HDF5\n",
    "# model.save_weights(\"mb-full.h5\")\n",
    "# print(\"Model saved to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='orange'>[!]  If using low-end GPU, UNCOMMENT this block</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"mb-full.h5\")\n",
    "\n",
    "# Compile model\n",
    "# model.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCfeub5auZK8"
   },
   "source": [
    "# <font color='green'>Evaluating</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GFzKxQtYuZK_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1206/1206 [==============================] - 437s 362ms/step\n",
      "78.93030047416687\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_sentences_X, cat_test_tags_y)\n",
    "print(scores[1] * 100)   # acc: 97.66269326210022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1206/1206 [==============================] - 179s 148ms/step\n",
      "F1-score: 3.6%\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "   di0fs0       0.00      0.00      0.00       308\n",
      "   da0ms0       0.00      0.00      0.00      1193\n",
      "  vmip3p0       0.01      0.02      0.01       281\n",
      "    sps00       0.00      0.00      0.00      5035\n",
      "      Fpa       0.00      0.00      0.00       184\n",
      "       cs       0.00      0.00      0.00       862\n",
      "  vmsi3s0       0.00      0.00      0.00        19\n",
      "  ncfp000       0.00      0.00      0.00       791\n",
      "   da0mp0       0.00      0.00      0.00       597\n",
      "       Fc       0.00      0.00      0.00      2258\n",
      "       cc       0.00      0.00      0.00      1141\n",
      "  ncms000       0.00      0.00      0.00      2249\n",
      "  ncfs000       0.00      0.00      0.00      2165\n",
      "      Fpt       0.00      0.00      0.00       181\n",
      "  vmn0000       0.00      0.00      0.00       789\n",
      "                1.00      1.00      1.00      1206\n",
      "  vmip3s0       0.00      0.00      0.00       661\n",
      "  np0000o       0.00      0.00      0.00       668\n",
      "   di0ms0       0.00      0.00      0.00       471\n",
      "  vmif3s0       0.00      0.00      0.00       118\n",
      "  vmis3s0       0.00      0.00      0.00       627\n",
      "   aq0mp0       0.00      0.00      0.00       186\n",
      "       rn       0.00      0.00      0.00       259\n",
      "  np0000a       0.00      0.00      0.00       194\n",
      "  nccp000       0.00      0.00      0.00       109\n",
      "      SUJ       0.00      0.00      0.00       748\n",
      "  vmsp3s0       0.00      0.00      0.00        76\n",
      "  np0000l       0.00      0.00      0.00       419\n",
      "  vmii3p0       0.00      0.00      0.00        46\n",
      "  ncmp000       0.00      0.00      0.00      1117\n",
      "   dn0cp0       0.00      0.00      0.00       140\n",
      "   da0fs0       0.00      0.00      0.00      1340\n",
      "  vmii3s0       0.00      0.00      0.00       127\n",
      "  np00000       0.00      0.00      0.00        53\n",
      "   aq0ms0       0.00      0.00      0.00       478\n",
      "       Fe       0.02      0.16      0.03       645\n",
      "       Fp       0.00      0.00      0.00      1159\n",
      " pr0cn000       0.00      0.00      0.00       614\n",
      "    spcms       0.00      0.00      0.00       667\n",
      "  vsn0000       0.00      0.00      0.00        46\n",
      "   da0fp0       0.00      0.00      0.00       400\n",
      "   aq0cp0       0.00      0.00      0.00       239\n",
      "  ncmn000       0.00      0.00      0.00        26\n",
      "   aq0cs0       0.00      0.00      0.00       573\n",
      "   aq0msp       0.00      0.00      0.00       203\n",
      " pn0cp000       0.00      0.00      0.00        22\n",
      "       rg       0.00      0.00      0.00      1074\n",
      "  nccs000       0.00      0.00      0.00       138\n",
      "   di0mp0       0.00      0.00      0.00       103\n",
      "   di0fp0       0.00      0.00      0.00        60\n",
      "  vaii3s0       0.00      0.00      0.00        29\n",
      "  np0000p       0.08      0.00      0.01       710\n",
      "  vsip3s0       0.00      0.00      0.00       183\n",
      "  vmp00pm       0.00      0.00      0.00        22\n",
      "  vsip3p0       0.00      0.00      0.00        51\n",
      "   dp3cs0       0.00      0.00      0.00       263\n",
      " pi0ms000       0.00      0.00      0.00        56\n",
      "  vsis3s0       0.00      0.00      0.00        47\n",
      "        Z       0.00      0.00      0.00       288\n",
      "   dp3cp0       0.00      0.00      0.00       106\n",
      "   dp2css       0.00      0.00      0.00         2\n",
      " pr0cs000       0.00      0.00      0.00        32\n",
      "       Fg       0.00      0.00      0.00       197\n",
      "  vmic3p0       0.00      0.00      0.00        13\n",
      "   dd0fs0       0.00      0.00      0.00        81\n",
      " p0300000       0.00      0.00      0.00       197\n",
      "   ao0ms0       0.00      0.00      0.00        55\n",
      " pp1cs000       0.00      0.00      0.00        19\n",
      "   aq0fs0       0.00      0.00      0.00       355\n",
      " p0000000       0.00      0.00      0.00       169\n",
      " pp3csd00       0.00      0.00      0.00        52\n",
      " pp3mp000       0.00      0.00      0.00        15\n",
      "      Fat       0.00      0.00      0.00         7\n",
      "   ao0fs0       0.00      0.00      0.00        68\n",
      "   dd0mp0       0.00      0.00      0.00        29\n",
      "  vsii3s0       0.00      0.00      0.00        25\n",
      " pt0cs000       0.00      0.00      0.00        28\n",
      "       Fx       0.00      0.07      0.00        54\n",
      "  vaip3s0       0.00      0.00      0.00       148\n",
      "  vmic3s0       0.00      0.00      0.00        21\n",
      "   di0cs0       0.00      0.00      0.00        28\n",
      "   aq0fp0       0.00      0.00      0.00       143\n",
      "  vmg0000       0.00      0.00      0.00        98\n",
      " pr000000       0.00      0.00      0.00        34\n",
      "  vasi3s0       0.00      0.00      0.00         6\n",
      "   dd0ms0       0.00      0.00      0.00        82\n",
      "       Fd       0.00      0.00      0.00        81\n",
      "   dn0mp0       0.00      0.00      0.00         4\n",
      "   da0ns0       0.00      0.00      0.00       117\n",
      "  vaii3p0       0.00      0.00      0.00         9\n",
      "   aq0mpp       0.00      0.00      0.00        89\n",
      "  vmii1s0       0.00      0.00      0.00         5\n",
      "  vaif3p0       0.00      0.00      0.00         1\n",
      "       Zp       0.00      0.00      0.00        43\n",
      " pd0fs000       0.00      0.00      0.00         6\n",
      "        W       0.00      0.02      0.00       203\n",
      "  vmp00sm       0.00      0.00      0.00       248\n",
      "  vsp00sm       0.00      0.00      0.00        21\n",
      "  vmis3p0       0.00      0.00      0.00       144\n",
      "  vmip1p0       0.00      0.00      0.00        36\n",
      "   aq0fsp       0.00      0.00      0.00       124\n",
      " pi0cs000       0.00      0.00      0.00        47\n",
      "  vasp3p0       0.00      0.00      0.00         1\n",
      "      Fit       0.00      0.00      0.00        35\n",
      "  vmm03s0       0.00      0.00      0.00         2\n",
      "  vmif3p0       0.00      0.00      0.00        43\n",
      " pp1csn00       0.00      0.00      0.00        15\n",
      "  vmip1s0       0.00      0.00      0.00        38\n",
      "       Zm       0.00      0.00      0.00        34\n",
      "   aq0fpp       0.00      0.00      0.00        64\n",
      "  vmsp3p0       0.00      0.00      0.00        41\n",
      "  vmsp1s0       0.00      0.00      0.00         9\n",
      "  nc00000       0.00      0.00      0.00        41\n",
      " pp3ms000       0.00      0.00      0.00        25\n",
      "       Fs       0.00      0.00      0.00        11\n",
      "  vsis3p0       0.00      0.00      0.00        18\n",
      "   ao0mp0       0.00      0.00      0.00        21\n",
      "  vmsi3p0       0.00      0.00      0.00        15\n",
      "  vaip1p0       0.00      0.00      0.00         9\n",
      " pr0mp000       0.00      0.00      0.00         2\n",
      " pi0fp000       0.00      0.00      0.00         6\n",
      "  van0000       0.00      0.00      0.00        20\n",
      "  vmp00pf       0.00      0.00      0.00         3\n",
      "  vmp00sf       0.00      0.00      0.00        12\n",
      "       Fz       0.00      0.00      0.00         8\n",
      " pn0mp000       0.00      0.00      0.00         4\n",
      " pp3cn000       0.00      0.00      0.00        18\n",
      "  vsg0000       0.00      0.00      0.00         7\n",
      " pi0mp000       0.00      0.00      0.00        18\n",
      " p020s000       0.00      0.00      0.00         4\n",
      " pi0fs000       0.00      0.00      0.00        12\n",
      " pp3fs000       0.00      0.00      0.00        13\n",
      "  vmii1p0       0.00      0.00      0.00         8\n",
      "  vaip3p0       0.00      0.00      0.00        40\n",
      "   aq0cn0       0.00      0.00      0.00        14\n",
      " pp2cso00       0.00      0.00      0.00         2\n",
      "        i       0.00      0.00      0.00        12\n",
      "  vaip1s0       0.00      0.00      0.00        10\n",
      " pp3fsa00       0.00      0.00      0.00        20\n",
      "  ncfn000       0.00      0.00      0.00        14\n",
      " pp1cp000       0.00      0.00      0.00        21\n",
      " pp3ns000       0.00      0.00      0.00        11\n",
      " pn0ms000       0.00      0.00      0.00         3\n",
      "  vasi1p0       0.00      0.00      0.00         1\n",
      "  vsif3s0       0.00      0.00      0.00        18\n",
      "   dd0cp0       0.00      0.00      0.00         3\n",
      "  vaii1p0       0.00      0.00      0.00         1\n",
      "  vsif3p0       0.00      0.00      0.00         6\n",
      "      Fia       0.00      0.00      0.00        12\n",
      "   dt0cn0       0.00      0.00      0.00         5\n",
      "   dp1mpp       0.00      0.00      0.00         3\n",
      " pi0cp000       0.00      0.00      0.00         4\n",
      " pp3msa00       0.00      0.00      0.00        34\n",
      " pd0ns000       0.00      0.00      0.00        27\n",
      "  vasp3s0       0.00      0.00      0.00         6\n",
      "  vsii3p0       0.00      0.00      0.00         7\n",
      "  vmic1p0       0.00      0.00      0.00         4\n",
      "  vmis1s0       0.00      0.00      0.00        12\n",
      " pp3mpa00       0.00      0.00      0.00         8\n",
      "   dn0ms0       0.00      0.00      0.00         4\n",
      "      Faa       0.00      0.00      0.00         4\n",
      "   dp3ms0       0.00      0.00      0.00         1\n",
      "  vsic1s0       0.00      0.00      0.00         2\n",
      "   ao0fp0       0.00      0.00      0.00        12\n",
      "  vmsp1p0       0.00      0.00      0.00         5\n",
      "  aq00000       0.00      0.00      0.00         2\n",
      " pp3cpd00       0.00      0.00      0.00        14\n",
      "  vsii1s0       0.00      0.00      0.00         2\n",
      "  vmip2s0       0.00      0.00      0.00        11\n",
      "  nccn000       0.00      0.00      0.00         2\n",
      " pp3fp000       0.00      0.00      0.00         4\n",
      " pp1mp000       0.00      0.00      0.00         8\n",
      " pp2cs00p       0.00      0.00      0.00         7\n",
      "   dp1css       0.00      0.00      0.00        11\n",
      "  vssp3p0       0.00      0.00      0.00         2\n",
      "   dd0fp0       0.00      0.00      0.00        22\n",
      " pp3cno00       0.00      0.00      0.00         6\n",
      "  vmm02s0       0.00      0.00      0.00         9\n",
      "   dn0fs0       0.00      0.00      0.00         5\n",
      " pr0cp000       0.00      0.00      0.00        14\n",
      "  vmis2s0       0.00      0.00      0.00         1\n",
      "  vmm03p0       0.00      0.00      0.00         4\n",
      " pp2csn00       0.00      0.00      0.00         2\n",
      " pt000000       0.00      0.00      0.00        11\n",
      "     sn.e       0.00      0.00      0.00         4\n",
      " pp3csa00       0.00      0.00      0.00         6\n",
      "  vmic1s0       0.00      0.00      0.00         1\n",
      "  vmif1p0       0.00      0.00      0.00         8\n",
      " pd0cs000       0.00      0.00      0.00         1\n",
      "   de0cn0       0.00      0.00      0.00         4\n",
      "   dd0cs0       0.00      0.00      0.00         3\n",
      " pr0ms000       0.00      0.00      0.00         4\n",
      "        X       0.00      0.00      0.00         1\n",
      " pd0mp000       0.00      0.00      0.00         6\n",
      "  vais3s0       0.00      0.00      0.00         7\n",
      "  vmm01p0       0.00      0.00      0.00         1\n",
      " pd0ms000       0.00      0.00      0.00        10\n",
      " pp3fpa00       0.00      0.00      0.00        10\n",
      "   dp1cps       0.00      0.00      0.00         2\n",
      " pp2cp00p       0.00      0.00      0.00         2\n",
      " pr0fs000       0.00      0.00      0.00         6\n",
      "  vsii1p0       0.00      0.00      0.00         1\n",
      " pd0fp000       0.00      0.33      0.00         3\n",
      "   dn0fp0       0.00      0.00      0.00         5\n",
      "  vmsi1s0       0.00      0.00      0.00         1\n",
      "  vaif3s0       0.00      0.00      0.00         6\n",
      "   di0cp0       0.00      0.00      0.00         5\n",
      "        Y       0.00      0.00      0.00         3\n",
      " p010s000       0.00      0.00      0.00         2\n",
      " pt0mp000       0.00      0.00      0.00         2\n",
      "   dp1fpp       0.00      0.00      0.00         3\n",
      "  vssp3s0       0.00      0.00      0.00        10\n",
      "   dp1fsp       0.00      0.00      0.00         5\n",
      "   dp1msp       0.00      0.00      0.00         6\n",
      " pp3cpa00       0.00      0.00      0.00         1\n",
      "   dn0cs0       0.00      0.00      0.00         1\n",
      " pd0cp000       0.00      0.00      0.00         1\n",
      "  vaic3s0       0.00      0.00      0.00         5\n",
      " p010p000       0.00      0.00      0.00         5\n",
      " pr0fp000       0.00      0.00      0.00         1\n",
      "  vmis1p0       0.00      0.00      0.00         1\n",
      "  vmsi1p0       0.00      0.00      0.00         1\n",
      "  vsif1s0       0.00      0.00      0.00         1\n",
      "  vsip2s0       0.00      0.00      0.00         3\n",
      "  vasi3p0       0.00      0.00      0.00         3\n",
      "  vsic3s0       0.00      0.00      0.00         4\n",
      "  vssi3p0       0.00      0.00      0.00         1\n",
      "  vap00sm       0.00      0.00      0.00         2\n",
      "  vsip1s0       0.00      0.00      0.00         4\n",
      "  vssi3s0       0.00      0.00      0.00         2\n",
      " px1ms0p0       0.00      0.00      0.00         1\n",
      "  vsip1p0       0.00      0.00      0.00         1\n",
      "   dp1mss       0.00      0.00      0.00         1\n",
      " pp2cs000       0.00      0.00      0.00         2\n",
      " pp3cna00       0.00      0.00      0.00         2\n",
      " pp1cso00       0.00      0.00      0.00         3\n",
      "  vssp1s0       0.00      0.00      0.00         1\n",
      "      ATR       0.00      0.00      0.00         1\n",
      "  vaic3p0       0.00      0.00      0.00         1\n",
      " pt0cp000       0.00      0.00      0.00         1\n",
      "  vmii2s0       0.00      0.00      0.00         3\n",
      "       Fh       0.00      0.00      0.00         1\n",
      " px1fp0p0       0.00      0.00      0.00         1\n",
      " px1fs0p0       0.00      0.00      0.00         1\n",
      " pn0fp000       0.00      0.00      0.00         1\n",
      "  vmif2s0       0.00      0.00      0.00         1\n",
      "  vsic3p0       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.04      0.03      0.04     39163\n",
      "macro avg       0.03      0.03      0.03     39163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "X_te = test_sentences_X[:201*128]\n",
    "test_pred = model.predict(np.array(X_te), verbose=1)\n",
    "#4768/4768 [==============================] - 64s 13ms/step\n",
    "idx2tag = {i: w for w, i in tag2index.items()}\n",
    "def pred2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            p_i = np.argmax(p)\n",
    "            out_i.append(idx2tag[p_i].replace(\"PADword\", \"O\"))\n",
    "        out.append(out_i)\n",
    "    return out\n",
    "def test2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            out_i.append(idx2tag[p].replace(\"PADword\", \"O\"))\n",
    "        out.append(out_i)\n",
    "    return out\n",
    "    \n",
    "pred_labels = pred2label(test_pred)\n",
    "test_labels = test2label(test_tags_y[:201*128])\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels)))\n",
    "print(classification_report(test_labels, pred_labels))\n",
    "#print(X_te[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Qbe0SmiuZLG"
   },
   "outputs": [],
   "source": [
    "# def plot_model_performance(train_loss, train_acc, train_val_loss, train_val_acc):\n",
    "#     \"\"\" Plot model loss and accuracy through epochs. \"\"\"\n",
    "#     blue= '#34495E'\n",
    "#     green = '#2ECC71'\n",
    "#     orange = '#E23B13'\n",
    "    \n",
    "#     # plot model loss\n",
    "#     fig, (ax1, ax2) = plt.subplots(2, figsize=(10, 8))\n",
    "#     ax1.plot(range(1, len(train_loss) + 1), train_loss, blue, linewidth=5, label='training')\n",
    "#     ax1.plot(range(1, len(train_val_loss) + 1), train_val_loss, green, linewidth=5, label='validation')\n",
    "#     ax1.set_xlabel('# epoch')\n",
    "#     ax1.set_ylabel('loss')\n",
    "#     ax1.tick_params('y')\n",
    "#     ax1.legend(loc='upper right', shadow=False)\n",
    "#     ax1.set_title('Model loss through #epochs', color=orange, fontweight='bold')\n",
    "    \n",
    "#     # plot model accuracy\n",
    "#     ax2.plot(range(1, len(train_acc) + 1), train_acc, blue, linewidth=5, label='training')\n",
    "#     ax2.plot(range(1, len(train_val_acc) + 1), train_val_acc, green, linewidth=5, label='validation')\n",
    "#     ax2.set_xlabel('# epoch')\n",
    "#     ax2.set_ylabel('accuracy')\n",
    "#     ax2.tick_params('y')\n",
    "#     ax2.legend(loc='lower right', shadow=False)\n",
    "#     ax2.set_title('Model accuracy through #epochs', color=orange, fontweight='bold')\n",
    "    \n",
    "# plot_model_performance(\n",
    "#     train_loss=model_hist.history.get('loss', []),\n",
    "#     train_acc=model_hist.history.get('crf_viterbi_accuracy', []),\n",
    "#     train_val_loss=model_hist.history.get('val_loss', []),\n",
    "#     train_val_acc=model_hist.history.get('val_crf_viterbi_accuracy', [])\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1yWdiaCduZLR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tags predicted for evaluating:\n",
      "\n",
      " ['Fe', 'vssi3p0', 'Fe', 'Fx', 'Fx', 'Fx', 'Y', 'Y', 'Fx', 'Fx', 'pp3fpa00', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# función que recibe en sequences la lista de oraciones donde cada elemento de la oracion es un one-hot vector\n",
    "# permite convertir Indices en Tags\n",
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences\n",
    "\n",
    "# Prediccion sobre el conjunto de pruebas. De la distribución probabilística a etiquetas\n",
    "prediction = model.predict(test_sentences_X)\n",
    "log_tokens = logits_to_tokens(prediction, {i: t for t, i in tag2index.items()})\n",
    "\n",
    "print(\"\\ntags predicted for evaluating:\\n\\n\", log_tokens[0], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zSE-XrafuZLa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "results:\n",
      "\n",
      "       Expected Predicted\n",
      "1       da0ms0        Fe\n",
      "2      nccs000   vssi3p0\n",
      "3      vmii3s0        Fe\n",
      "4        sps00        Fx\n",
      "5       dp3cs0        Fx\n",
      "...        ...       ...\n",
      "38132       rg  pp3ms000\n",
      "38133   da0fs0       Fpa\n",
      "38134  ncfs000   vmip2s0\n",
      "38135   aq0cs0   vsis3p0\n",
      "38136       Fp   vsis3p0\n",
      "\n",
      "[38136 rows x 2 columns]\n",
      "\n",
      "\n",
      "classification_report:\n",
      "\n",
      "            precision    recall  f1-score   support\n",
      "\n",
      "      FpO       0.00      0.00      0.00      1155\n",
      "      FcO       0.00      0.00      0.00      2258\n",
      "  aq0fs0O       0.00      0.00      0.00       355\n",
      " vmp00smO       0.00      0.00      0.00       248\n",
      "   spcmsO       0.00      0.00      0.00       667\n",
      "  di0fs0O       0.00      0.00      0.00       308\n",
      "      FeO       0.02      0.16      0.03       645\n",
      " vmip1p0O       0.00      0.00      0.00        36\n",
      " vmip3s0O       0.00      0.00      0.00       661\n",
      "   sps00O       0.00      0.00      0.00      5035\n",
      " np0000pO       0.08      0.00      0.01       709\n",
      "  da0fp0O       0.00      0.00      0.00       400\n",
      "       WO       0.00      0.02      0.00       203\n",
      "      FgO       0.00      0.00      0.00       197\n",
      " vmic3p0O       0.00      0.00      0.00        13\n",
      " ncms000O       0.00      0.00      0.00      2249\n",
      "  de0cn0O       0.00      0.00      0.00         4\n",
      "  dd0fs0O       0.00      0.00      0.00        81\n",
      "  da0fs0O       0.00      0.00      0.00      1340\n",
      " vmif1p0O       0.00      0.00      0.00         8\n",
      " vmn0000O       0.00      0.00      0.00       789\n",
      "  aq0fspO       0.00      0.00      0.00       124\n",
      " vmis3s0O       0.00      0.00      0.00       627\n",
      "       ZO       0.00      0.00      0.00       288\n",
      "      csO       0.00      0.00      0.00       862\n",
      "     FpaO       0.00      0.00      0.00       184\n",
      "pi0ms000O       0.00      0.00      0.00        56\n",
      " ncfs000O       0.00      0.00      0.00      2165\n",
      "      ccO       0.00      0.00      0.00      1141\n",
      "pp3msa00O       0.00      0.00      0.00        34\n",
      "     SUJO       0.00      0.00      0.00       747\n",
      " np0000oO       0.00      0.00      0.00       668\n",
      "pr0cn000O       0.00      0.00      0.00       614\n",
      "pi0mp000O       0.00      0.00      0.00        18\n",
      " vsip3s0O       0.00      0.00      0.00       183\n",
      " nccp000O       0.00      0.00      0.00       109\n",
      "      rgO       0.00      0.00      0.00      1074\n",
      "      FxO       0.00      0.07      0.00        54\n",
      "  da0mp0O       0.00      0.00      0.00       597\n",
      "pp1mp000O       0.00      0.00      0.00         8\n",
      "      rnO       0.00      0.00      0.00       259\n",
      "  aq0ms0O       0.00      0.00      0.00       478\n",
      "  da0ms0O       0.00      0.00      0.00      1193\n",
      " ncfp000O       0.00      0.00      0.00       791\n",
      " vsii1p0O       0.00      0.00      0.00         1\n",
      "  di0ms0O       0.00      0.00      0.00       471\n",
      " vmic3s0O       0.00      0.00      0.00        21\n",
      " ncmp000O       0.00      0.00      0.00      1117\n",
      "     FptO       0.00      0.00      0.00       181\n",
      " vmsp3s0O       0.00      0.00      0.00        76\n",
      "  aq0cp0O       0.00      0.00      0.00       239\n",
      " vmip3p0O       0.01      0.02      0.01       281\n",
      " np0000lO       0.00      0.00      0.00       419\n",
      "  aq0cs0O       0.00      0.00      0.00       573\n",
      " ncmn000O       0.00      0.00      0.00        26\n",
      "      FdO       0.00      0.00      0.00        81\n",
      " vaip3s0O       0.00      0.00      0.00       148\n",
      "p010s000O       0.00      0.00      0.00         2\n",
      "p0300000O       0.00      0.00      0.00       197\n",
      "  aq0mp0O       0.00      0.00      0.00       186\n",
      "  dd0ms0O       0.00      0.00      0.00        82\n",
      "  aq0fp0O       0.00      0.00      0.00       143\n",
      "pp3cpd00O       0.00      0.00      0.00        14\n",
      " nc00000O       0.00      0.00      0.00        41\n",
      "      ZmO       0.00      0.00      0.00        34\n",
      "pp3csd00O       0.00      0.00      0.00        52\n",
      " vais3s0O       0.00      0.00      0.00         7\n",
      "pp3ms000O       0.00      0.00      0.00        25\n",
      "  dp3cp0O       0.00      0.00      0.00       106\n",
      "p0000000O       0.00      0.00      0.00       169\n",
      "  ao0ms0O       0.00      0.00      0.00        55\n",
      " vmif3s0O       0.00      0.00      0.00       118\n",
      "  dd0fp0O       0.00      0.00      0.00        22\n",
      " vsip1s0O       0.00      0.00      0.00         4\n",
      " vaii3p0O       0.00      0.00      0.00         9\n",
      " vmsi3s0O       0.00      0.00      0.00        19\n",
      " vmis1s0O       0.00      0.00      0.00        12\n",
      "       iO       0.00      0.00      0.00        12\n",
      " np0000aO       0.00      0.00      0.00       194\n",
      "  dp3cs0O       0.00      0.00      0.00       263\n",
      "pi0cs000O       0.00      0.00      0.00        47\n",
      " vsip3p0O       0.00      0.00      0.00        51\n",
      "  di0fp0O       0.00      0.00      0.00        60\n",
      "      ZpO       0.00      0.00      0.00        43\n",
      "     FitO       0.00      0.00      0.00        35\n",
      " vsis3s0O       0.00      0.00      0.00        47\n",
      "pp1cs000O       0.00      0.00      0.00        19\n",
      " vmsp3p0O       0.00      0.00      0.00        41\n",
      "pt0cs000O       0.00      0.00      0.00        28\n",
      " vaip3p0O       0.00      0.00      0.00        40\n",
      "  da0ns0O       0.00      0.00      0.00       117\n",
      "  aq0mspO       0.00      0.00      0.00       203\n",
      " vmm01p0O       0.00      0.00      0.00         1\n",
      "pr0cs000O       0.00      0.00      0.00        32\n",
      " vasp3s0O       0.00      0.00      0.00         6\n",
      " np00000O       0.00      0.00      0.00        53\n",
      " vsp00smO       0.00      0.00      0.00        21\n",
      " vmii3p0O       0.00      0.00      0.00        46\n",
      "pp2cs00pO       0.00      0.00      0.00         7\n",
      " vmif3p0O       0.00      0.00      0.00        43\n",
      "  dn0cp0O       0.00      0.00      0.00       140\n",
      "pi0fp000O       0.00      0.00      0.00         6\n",
      " vsii3s0O       0.00      0.00      0.00        25\n",
      " vmii3s0O       0.00      0.00      0.00       127\n",
      "  aq0mppO       0.00      0.00      0.00        89\n",
      " vaii3s0O       0.00      0.00      0.00        29\n",
      " vmis3p0O       0.00      0.00      0.00       144\n",
      " nccs000O       0.00      0.00      0.00       138\n",
      "px1ms0p0O       0.00      0.00      0.00         1\n",
      " vsif3s0O       0.00      0.00      0.00        18\n",
      "pp1cso00O       0.00      0.00      0.00         3\n",
      " vsg0000O       0.00      0.00      0.00         7\n",
      "  dd0cp0O       0.00      0.00      0.00         3\n",
      " vsis3p0O       0.00      0.00      0.00        18\n",
      " vsn0000O       0.00      0.00      0.00        46\n",
      "pp3fp000O       0.00      0.00      0.00         4\n",
      "  di0cs0O       0.00      0.00      0.00        28\n",
      " vaic3s0O       0.00      0.00      0.00         5\n",
      "  ao0fs0O       0.00      0.00      0.00        68\n",
      "  ao0mp0O       0.00      0.00      0.00        21\n",
      " vmip1s0O       0.00      0.00      0.00        38\n",
      "pp3fpa00O       0.00      0.00      0.00        10\n",
      " vaif3s0O       0.00      0.00      0.00         6\n",
      "  aq0fppO       0.00      0.00      0.00        64\n",
      "pp3fsa00O       0.00      0.00      0.00        20\n",
      " vmp00pmO       0.00      0.00      0.00        22\n",
      " vmg0000O       0.00      0.00      0.00        98\n",
      "     FatO       0.00      0.00      0.00         7\n",
      "pd0ns000O       0.00      0.00      0.00        27\n",
      " vaip1s0O       0.00      0.00      0.00        10\n",
      "     FiaO       0.00      0.00      0.00        12\n",
      "pp3cn000O       0.00      0.00      0.00        18\n",
      " vmii2s0O       0.00      0.00      0.00         3\n",
      "      FsO       0.00      0.00      0.00        11\n",
      "  dp1fspO       0.00      0.00      0.00         5\n",
      " vmii1p0O       0.00      0.00      0.00         8\n",
      "  dd0cs0O       0.00      0.00      0.00         3\n",
      "pp3mp000O       0.00      0.00      0.00        15\n",
      "  ao0fp0O       0.00      0.00      0.00        12\n",
      " vmsi3p0O       0.00      0.00      0.00        15\n",
      "  dn0ms0O       0.00      0.00      0.00         4\n",
      "  dp1cssO       0.00      0.00      0.00        11\n",
      "  dd0mp0O       0.00      0.00      0.00        29\n",
      "pr000000O       0.00      0.00      0.00        34\n",
      " ncfn000O       0.00      0.00      0.00        14\n",
      " vmm03s0O       0.00      0.00      0.00         2\n",
      " vssp3s0O       0.00      0.00      0.00        10\n",
      " vsic3s0O       0.00      0.00      0.00         4\n",
      "pr0fs000O       0.00      0.00      0.00         6\n",
      " vmm03p0O       0.00      0.00      0.00         4\n",
      "p010p000O       0.00      0.00      0.00         5\n",
      "pp1cp000O       0.00      0.00      0.00        21\n",
      " vmm02s0O       0.00      0.00      0.00         9\n",
      "  di0mp0O       0.00      0.00      0.00       103\n",
      "pp3fs000O       0.00      0.00      0.00        13\n",
      "pp3mpa00O       0.00      0.00      0.00         8\n",
      "      FzO       0.00      0.00      0.00         8\n",
      " van0000O       0.00      0.00      0.00        20\n",
      "    sn.eO       0.00      0.00      0.00         4\n",
      " aq00000O       0.00      0.00      0.00         2\n",
      " vmip2s0O       0.00      0.00      0.00        11\n",
      " vmp00sfO       0.00      0.00      0.00        12\n",
      "pi0fs000O       0.00      0.00      0.00        12\n",
      " vasi3s0O       0.00      0.00      0.00         6\n",
      "pd0ms000O       0.00      0.00      0.00        10\n",
      "  dp1fppO       0.00      0.00      0.00         3\n",
      "pn0fp000O       0.00      0.00      0.00         1\n",
      "pp3cno00O       0.00      0.00      0.00         6\n",
      "pp3ns000O       0.00      0.00      0.00        11\n",
      "     FaaO       0.00      0.00      0.00         4\n",
      " vsii1s0O       0.00      0.00      0.00         2\n",
      "pd0cs000O       0.00      0.00      0.00         1\n",
      "pr0cp000O       0.00      0.00      0.00        14\n",
      " vssi3s0O       0.00      0.00      0.00         2\n",
      "pt0cp000O       0.00      0.00      0.00         1\n",
      "pt000000O       0.00      0.00      0.00        11\n",
      " vmis1p0O       0.00      0.00      0.00         1\n",
      "pp1csn00O       0.00      0.00      0.00        15\n",
      " vaip1p0O       0.00      0.00      0.00         9\n",
      " vap00smO       0.00      0.00      0.00         2\n",
      " vmsp1s0O       0.00      0.00      0.00         9\n",
      " vmsp1p0O       0.00      0.00      0.00         5\n",
      "pd0fs000O       0.00      0.00      0.00         6\n",
      "pn0cp000O       0.00      0.00      0.00        22\n",
      "pp2cp00pO       0.00      0.00      0.00         2\n",
      "pn0mp000O       0.00      0.00      0.00         4\n",
      " vsif3p0O       0.00      0.00      0.00         6\n",
      " vsip2s0O       0.00      0.00      0.00         3\n",
      "  dt0cn0O       0.00      0.00      0.00         5\n",
      "pr0ms000O       0.00      0.00      0.00         4\n",
      "pp2cs000O       0.00      0.00      0.00         2\n",
      " vmsi1s0O       0.00      0.00      0.00         1\n",
      "  dn0fp0O       0.00      0.00      0.00         5\n",
      "pr0mp000O       0.00      0.00      0.00         2\n",
      "  aq0cn0O       0.00      0.00      0.00        14\n",
      "  dn0fs0O       0.00      0.00      0.00         5\n",
      " vmii1s0O       0.00      0.00      0.00         5\n",
      "  dp1mppO       0.00      0.00      0.00         3\n",
      " vsii3p0O       0.00      0.00      0.00         7\n",
      "pp3csa00O       0.00      0.00      0.00         6\n",
      " vmsi1p0O       0.00      0.00      0.00         1\n",
      " vssp1s0O       0.00      0.00      0.00         1\n",
      " vmic1p0O       0.00      0.00      0.00         4\n",
      "p020s000O       0.00      0.00      0.00         4\n",
      "  di0cp0O       0.00      0.00      0.00         5\n",
      " vssp3p0O       0.00      0.00      0.00         2\n",
      "pn0ms000O       0.00      0.00      0.00         3\n",
      "  dp1cpsO       0.00      0.00      0.00         2\n",
      " vsic3p0O       0.00      0.00      0.00         1\n",
      "pt0mp000O       0.00      0.00      0.00         2\n",
      " vaii1p0O       0.00      0.00      0.00         1\n",
      "       YO       0.00      0.00      0.00         3\n",
      "pp3cna00O       0.00      0.00      0.00         2\n",
      " vmp00pfO       0.00      0.00      0.00         3\n",
      "  dp3ms0O       0.00      0.00      0.00         1\n",
      "      FhO       0.00      0.00      0.00         1\n",
      "pp2cso00O       0.00      0.00      0.00         2\n",
      "pd0fp000O       0.00      0.33      0.00         3\n",
      "pp2csn00O       0.00      0.00      0.00         2\n",
      "pd0mp000O       0.00      0.00      0.00         6\n",
      "px1fp0p0O       0.00      0.00      0.00         1\n",
      " vasi3p0O       0.00      0.00      0.00         3\n",
      "  dp1mspO       0.00      0.00      0.00         6\n",
      "pi0cp000O       0.00      0.00      0.00         4\n",
      "  dn0mp0O       0.00      0.00      0.00         4\n",
      " vsic1s0O       0.00      0.00      0.00         2\n",
      "       XO       0.00      0.00      0.00         1\n",
      "  dp2cssO       0.00      0.00      0.00         2\n",
      "     ATRO       0.00      0.00      0.00         1\n",
      " vaif3p0O       0.00      0.00      0.00         1\n",
      " vsif1s0O       0.00      0.00      0.00         1\n",
      " vasp3p0O       0.00      0.00      0.00         1\n",
      "pp3cpa00O       0.00      0.00      0.00         1\n",
      " vasi1p0O       0.00      0.00      0.00         1\n",
      " vmif2s0O       0.00      0.00      0.00         1\n",
      "  dn0cs0O       0.00      0.00      0.00         1\n",
      " vssi3p0O       0.00      0.00      0.00         1\n",
      " vaic3p0O       0.00      0.00      0.00         1\n",
      " vmic1s0O       0.00      0.00      0.00         1\n",
      "  dp1mssO       0.00      0.00      0.00         1\n",
      " vsip1p0O       0.00      0.00      0.00         1\n",
      " nccn000O       0.00      0.00      0.00         2\n",
      "px1fs0p0O       0.00      0.00      0.00         1\n",
      "pd0cp000O       0.00      0.00      0.00         1\n",
      "pr0fp000O       0.00      0.00      0.00         1\n",
      " vmis2s0O       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.00      0.00      0.00     37951\n",
      "macro avg       0.00      0.00      0.00     37951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Expected', 'Predicted'])\n",
    "k = 0\n",
    "for i, lista_etiquetas_oracion in enumerate(test_tags):\n",
    "    for j, etiquetas in enumerate(lista_etiquetas_oracion):\n",
    "        k = k + 1\n",
    "        results.loc[k, 'Expected'] = etiquetas\n",
    "        results.loc[k, 'Predicted'] = log_tokens[i][j]\n",
    "\n",
    "print(\"\\nresults:\\n\\n\", results)\n",
    "print('\\n\\nclassification_report:\\n\\n', classification_report(results['Expected'], results['Predicted']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LswNKDP6uZLf"
   },
   "source": [
    "# <font color='green'>Testing</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='orange'>[!]  If running online, COMMENT this block (Graphic Interface)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWoNvq1PuZLj"
   },
   "outputs": [],
   "source": [
    "def pos_tagging():\n",
    "#    sentence=\"Correr es importante para mi .\"\n",
    "#    sentence=\"El hombre bajo corre bajo el puente con bajo índice de adrenalina\"\n",
    "\n",
    "    test_samples = [inputText.get(\"1.0\",END).split()]  \n",
    "    test_samples_X = []\n",
    "    for s in test_samples:\n",
    "        s_int = []\n",
    "        for w in s:\n",
    "            try:\n",
    "                s_int.append(word2index[w.lower()])\n",
    "            except KeyError:\n",
    "                s_int.append(word2index['-OOV-'])\n",
    "\n",
    "        test_samples_X.append(s_int)\n",
    "\n",
    "    test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
    "    \n",
    "    predictions = model.predict(test_samples_X)\n",
    "    tags = predictions.shape[2]\n",
    "\n",
    "    heads = test_samples[0]\n",
    "    body = [log_tokens[0][:len(test_samples[0])]]\n",
    "    \n",
    "    index = 0  \n",
    "    output = PrettyTable(['Word', 'Tag'])\n",
    "    for w in heads:\n",
    "        output.add_row([w, body[0][index] ])       \n",
    "        index+=1\n",
    "    \n",
    "    output.add_row([\"                 \",\"                \"])\n",
    "    \n",
    "    #-------------------------------------\n",
    "    outputText.configure(state='normal')\n",
    "    outputText.delete('1.0', END)    \n",
    "    outputText.insert(\"insert\", output) \n",
    "    outputText.configure(state='disabled')\n",
    "    #-------------------------------------\n",
    "\n",
    "    return output\n",
    "\n",
    "window = Tk()\n",
    "window.title(\"Postagging\")\n",
    "window.geometry('770x650')\n",
    "title = '\\t\\tPostagging\\n BiLSTM + CRF para la clasificación de NER \\n\\tsobre el corpus Conll2002'\n",
    "\n",
    "label= Label( text = title, background=\"white\", font = \"Consolas 18 bold\")\n",
    "label.grid(column=0, row=0, pady=(10, 10), columnspan=2)\n",
    "\n",
    "btn = Button(window, text=\" POS TAGGING >> \",command=pos_tagging)\n",
    "btn.grid(column=0, row=1, pady=(10, 10), columnspan=2 )\n",
    "\n",
    "inputText = Text(window,height=25, width=40, font = \"Consolas 12\")\n",
    "inputText.grid(column=0, row=3, padx=(10,10))\n",
    "\n",
    "outputText = Text(window,height=25, width=40, state='disabled', font = \"Consolas 12\")\n",
    "outputText.grid(column=1, row=3, padx=(10,10))\n",
    "outputText.tag_configure(\"center\", justify='center')\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing sentences:\n",
      "\n",
      " ['Correr', 'es', 'importante', 'para', 'mi', '.'] \n",
      " ['El', 'hombre', 'bajo', 'corre', 'bajo', 'el', 'puente', 'con', 'bajo', 'índice', 'de', 'adrenalina', '.']\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Testing sentences parsed to integer (matrix of  2  sentences X  149  words ):\n",
      "\n",
      "\n",
      "[ 2738   506  4191 22486 16714 17042     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0] \n",
      "\n",
      "[ 5893  2359   128 10016   128  5893  7453 22324   128 23966  7145     1\n",
      " 17042     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_samples = [\n",
    "    \"Correr es importante para mi .\".split(),\n",
    "    \"El hombre bajo corre bajo el puente con bajo índice de adrenalina .\".split()\n",
    "]\n",
    "print(\"\\nTesting sentences:\\n\\n\", test_samples[0], \"\\n\", test_samples[1])\n",
    "\n",
    "test_samples_X = []\n",
    "for s in test_samples:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    "    \n",
    "    test_samples_X.append(s_int)\n",
    "\n",
    "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
    "sentences,words = test_samples_X.shape\n",
    "print(\"\\n----------------------------------------------------------------------------\\n\")\n",
    "print(\"\\nTesting sentences parsed to integer (matrix of \", sentences ,\" sentences X \", words ,\" words ):\\n\\n\")\n",
    "print(test_samples_X[0], \"\\n\")\n",
    "print(test_samples_X[1], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test results:\n",
      "\n",
      "\n",
      "Correr    es       importante    para    mi    .\n",
      "--------  -------  ------------  ------  ----  ---\n",
      "Fe        vssi3p0  Fe            Fx      Fx    Fx \n",
      "\n",
      "El    hombre    bajo     corre    bajo    el        puente    con    bajo      índice    de        adrenalina    .\n",
      "----  --------  -------  -------  ------  --------  --------  -----  --------  --------  --------  ------------  -----\n",
      "Fpa   Fpa       vssi3p0  Fe       Fx      pp3cn000  Fx        W      pd0fp000  Fe        pp3fpa00  -PAD-         -PAD- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "heads1 = test_samples[0]\n",
    "body1 = [log_tokens[0][:len(test_samples[0])]]\n",
    "\n",
    "heads2 = test_samples[1]\n",
    "body2 = [log_tokens[1][:len(test_samples[1])]]\n",
    "\n",
    "print(\"\\nTest results:\\n\\n\")\n",
    "print(tabulate(body1, headers=heads1), \"\\n\")\n",
    "print(tabulate(body2, headers=heads2), \"\\n\")\n",
    "\n",
    "## postagging Freeling 4.1\n",
    "\n",
    "## El      hombre   bajo     corre    bajo  el      puente   con  bajo  índice   de  adrenalina  .\n",
    "## DA0MS0  NCMS000  AQ0MS00  VMIP3S0  SP    DA0MS0  NCMS000  SP   SP    NCMS000  SP  NCFS000     Fp\n",
    "\n",
    "\n",
    "## pos tagger Stanford NLP\n",
    "\n",
    "## El      hombre   bajo     corre    bajo  el      puente   con    bajo   índice  de    adrenalina  .\n",
    "## da0000  nc0s000  aq0000   vmip000  sp000 da0000  nc0s000  sp000  aq0000 nc0s000 sp000 nc0s000     fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pos_tagging2_version_3_.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
